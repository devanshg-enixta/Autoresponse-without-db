{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import unicodedata\n",
    "import glob\n",
    "from itertools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_file = pd.read_csv('source_reviews_for_response_generation_2018-05-12 17_19_16.919510.csv',encoding='utf-8',quoting=csv.QUOTE_ALL)\n",
    "review_file.dropna(subset=['review_text'],inplace=True)\n",
    "#review_file.source_review_id = review_file.source_review_id.astype(str)\n",
    "#review_file.id = review_file.id.astype(int)\n",
    "#review_file.id = review_file.id.astype(str)\n",
    "\n",
    "#review_file.to_csv('source_reviews_for_response_generation_2018-04-24 11_59_28.650544.csv',encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, re, sys, fnmatch, string\n",
    "import pandas as pd\n",
    "import unicodecsv as csv\n",
    "import enchant\n",
    "import os\n",
    "reload(sys)\n",
    "d = enchant.Dict(\"en_US\")\n",
    "negative = []\n",
    "positive = []\n",
    "neutral = []\n",
    "compounded = []\n",
    "total1=[]\n",
    "rev_id=[]\n",
    "subject=[]\n",
    "alt_subject=[]\n",
    "f = 'vader_sentiment_lexicon.txt' # empirically derived valence ratings for words, emoticons, slang, swear words, acronyms/initialisms\n",
    "\n",
    "word_valence_dict = dict(map(lambda (w, m): (w, float(m)), [\n",
    "            wmsr.strip().split('\\t')[0:2] for wmsr in open(f) ]))\n",
    "\n",
    "#except:\n",
    "#    word_valence_dict = dict(map(lambda (w, m): (w, float(m)), [\n",
    "#            wmsr.strip().split('        ')[0:2] for wmsr in open(f) ]))\n",
    "\n",
    "# for removing punctuation\n",
    "regex_remove_punctuation = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "def sentiment(text):\n",
    "    \"\"\"\n",
    "    Returns a float for sentiment strength based on the input text.\n",
    "    Positive values are positive valence, negative value are negative valence.\n",
    "    \"\"\"\n",
    "    text = text.lower().replace('\"','')\n",
    "    test =text.split(' ')\n",
    "    length = len(test)\n",
    "    #print length\n",
    "    for i in range (0,len(test)-1):\n",
    "        if '/' in test[i]:\n",
    "            test[i]=test[i].split('/')[0]\n",
    "\n",
    "        if test[i] == \"five\" or test[i] == \"one\" or test[i] == \"four\" or test[i] == \"three\" or test[i] == \"two\" or test[i] == \"5\" or test[i] == \"4\" or test[i] == \"3\" or test[i] == \"2\" or test[i] == \"1\" or test[i] == \"no\" or test[i] == \"open\" :\n",
    "            if test[i+1] =='star' or test[i+1] == 'stars' or test[i+1] == 'more'or test[i+1] == 'pack'or test[i+1] == 'packet'or test[i+1] == 'package':\n",
    "                test[i]=test[i] + '-' + test[i+1]\n",
    "                test [i+1]=''\n",
    "        if test[i] == \"stomach\" or test[i] == \"loose\":\n",
    "            if test[i+1] =='ace' or test[i+1] == 'motion':\n",
    "                test[i]=test[i] + '-' + test[i+1]\n",
    "                test [i+1]=''\n",
    "        if test[i] == \"use\":\n",
    "            if test[i+1] =='less' or test[i+1] == 'full' or test[i+1] == 'les' or test[i+1] == 'lss' or test[i+1] == 'ls':\n",
    "                test[i]=test[i] + '-' + test[i+1]\n",
    "                test [i+1]=''\n",
    "\n",
    "        if test[i] == \"die\" :\n",
    "            if test[i+1] =='to':\n",
    "                test[i]=test[i] + '-' + test[i+1]\n",
    "                if test[i+2]:\n",
    "                    test[i]=test[i] + '-' + test[i+2]\n",
    "                    test [i+2]=''\n",
    "        try:\n",
    "            if not d.check(test[i]):\n",
    "                test[i]=d.suggest(test[i])[0]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    text = \"  \".join(test)\n",
    "    text = text.strip()\n",
    "    wordsAndEmoticons = str(text).split() #doesn't separate words from adjacent punctuation (keeps emoticons & contractions)\n",
    "#    print wordsAndEmoticons\n",
    "    text_mod = text #regex_remove_punctuation.sub('', text) # removes punctuation (but loses emoticons & contractions)\n",
    " #   print text_mod\n",
    "    wordsOnly = str(text_mod).split()\n",
    " #   print wordsOnly\n",
    "    # get rid of empty items or single letter \"words\" like 'a' and 'I' from wordsOnly\n",
    "    for word in wordsOnly:\n",
    "        if len(word) <= 1:\n",
    "            wordsOnly.remove(word)    \n",
    "    # now remove adjacent & redundant punctuation from [wordsAndEmoticons] while keeping emoticons and contractions\n",
    "    puncList = [\".\", \"!\", \"?\", \",\", \";\", \":\", \"-\", \"'\", \"\\\"\", \n",
    "                \"!!\", \"!!!\", \"??\", \"???\", \"?!?\", \"!?!\", \"?!?!\", \"!?!?\"] \n",
    "    for word in wordsOnly:\n",
    "        for p in puncList:\n",
    "            pword = p + word\n",
    "            x1 = wordsAndEmoticons.count(pword)\n",
    "            while x1 > 0:\n",
    "                i = wordsAndEmoticons.index(pword)\n",
    "                wordsAndEmoticons.remove(pword)\n",
    "                wordsAndEmoticons.insert(i, word)\n",
    "                x1 = wordsAndEmoticons.count(pword)\n",
    "            \n",
    "            wordp = word + p\n",
    "            x2 = wordsAndEmoticons.count(wordp)\n",
    "            while x2 > 0:\n",
    "                i = wordsAndEmoticons.index(wordp)\n",
    "                wordsAndEmoticons.remove(wordp)\n",
    "                wordsAndEmoticons.insert(i, word)\n",
    "                x2 = wordsAndEmoticons.count(wordp)\n",
    "\n",
    "    # get rid of residual empty items or single letter \"words\" like 'a' and 'I' from wordsAndEmoticons\n",
    "    for word in wordsAndEmoticons:\n",
    "        if len(word) <= 1:\n",
    "            wordsAndEmoticons.remove(word)\n",
    "    word_len= len(wordsAndEmoticons)\n",
    "    #print wordsAndEmoticons   \n",
    "    # remove stopwords from [wordsAndEmoticons]\n",
    "    #stopwords = [str(word).strip() for word in open('stopwords.txt')]\n",
    "    #for word in wordsAndEmoticons:\n",
    "    #    if word in stopwords:\n",
    "    #        wordsAndEmoticons.remove(word)\n",
    "    \n",
    "    # check for negation\n",
    "    negate = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    "              \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    "              \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    "              \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    "              \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\",\"no\", \"nothing\", \"nowhere\", \n",
    "              \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    "              \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",  \n",
    "              \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\",\"less\"]\n",
    "    def negated(list, nWords=[], includeNT=True):\n",
    "        nWords.extend(negate)\n",
    "        #print list\n",
    "        for word in nWords:\n",
    "            if word in list:\n",
    "                #print word\n",
    "                return True\n",
    "        if includeNT:\n",
    "            for word in list:\n",
    "                if \"n't\" in word:\n",
    "                    return True\n",
    "        if \"least\" in list:\n",
    "            i = list.index(\"least\")\n",
    "            if i > 0 and list[i-1] != \"at\":\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def normalize(score, alpha=1):\n",
    "        # normalize the score to be between -1 and 1 using an alpha that approximates the max expected value \n",
    "        normScore = score/math.sqrt( ((score*score) + alpha) )\n",
    "        return normScore\n",
    "    \n",
    "    def wildCardMatch(patternWithWildcard, listOfStringsToMatchAgainst):\n",
    "        listOfMatches = fnmatch.filter(listOfStringsToMatchAgainst, patternWithWildcard)\n",
    "        return listOfMatches\n",
    "        \n",
    "    \n",
    "    def isALLCAP_differential(wordList):\n",
    "        countALLCAPS= 0\n",
    "        for w in wordList:\n",
    "            if str(w).isupper(): \n",
    "                countALLCAPS += 1\n",
    "        cap_differential = len(wordList) - countALLCAPS\n",
    "        if cap_differential > 0 and cap_differential < len(wordList):\n",
    "            isDiff = True\n",
    "        else: isDiff = False\n",
    "        return isDiff\n",
    "    isCap_diff = isALLCAP_differential(wordsAndEmoticons)\n",
    "    \n",
    "    b_incr = 0.293 #(empirically derived mean sentiment intensity rating increase for booster words)\n",
    "    b_decr = -0.293\n",
    "    # booster/dampener 'intensifiers' or 'degree adverbs' http://en.wiktionary.org/wiki/Category:English_degree_adverbs\n",
    "    booster_dict = {\"absolutely\": b_incr, \"amazingly\": b_incr, \"awfully\": b_incr, \"completely\": b_incr, \"considerably\": b_incr, \n",
    "                    \"decidedly\": b_incr, \"deeply\": b_incr, \"effing\": b_incr, \"enormously\": b_incr, \n",
    "                    \"entirely\": b_incr, \"especially\": b_incr, \"exceptionally\": b_incr, \"extremely\": b_incr,\n",
    "                    \"fabulously\": b_incr, \"flipping\": b_incr, \"flippin\": b_incr, \n",
    "                    \"fricking\": b_incr, \"frickin\": b_incr, \"frigging\": b_incr, \"friggin\": b_incr, \"fully\": b_incr, \"fucking\": b_incr, \n",
    "                    \"greatly\": b_incr, \"hella\": b_incr, \"highly\": b_incr, \"hugely\": b_incr, \"incredibly\": b_incr, \n",
    "                    \"intensely\": b_incr, \"majorly\": b_incr, \"more\": b_incr, \"most\": b_incr, \"must\": b_incr, \"particularly\": b_incr, \n",
    "                    \"purely\": b_incr,\"please\": b_incr, \"quite\": b_incr, \"really\": b_incr, \"remarkably\": b_incr, \n",
    "                    \"so\": b_incr,  \"substantially\": b_incr, \"damn\": b_incr, \n",
    "                    \"thoroughly\": b_incr, \"totally\": b_incr, \"tremendously\": b_incr, \n",
    "                    \"uber\": b_incr, \"unbelievably\": b_incr, \"unusually\": b_incr, \"utterly\": b_incr, \n",
    "                    \"very\": b_incr,\"big\":b_incr,\"help\":b_incr,\"heck\":b_incr,\"too\":b_incr,\"lots\":b_incr,\"atall\":b_incr,\n",
    "                    \n",
    "                    \"almost\": b_decr, \"barely\": b_decr, \"hardly\": b_decr, \"just enough\": b_decr, \n",
    "                    \"kind of\": b_decr, \"kinda\": b_decr, \"kindof\": b_decr, \"kind-of\": b_decr,\n",
    "                    \"less\": b_decr, \"little\": b_decr, \"marginally\": b_decr, \"occasionally\": b_decr, \"partly\": b_decr, \n",
    "                    \"scarcely\": b_decr, \"slightly\": b_decr, \"somewhat\": b_decr, \n",
    "                    \"sort of\": b_decr, \"sorta\": b_decr, \"sortof\": b_decr, \"sort-of\": b_decr,\"rip\": b_decr}\n",
    "    sentiments = []\n",
    " #   print wordsAndEmoticons\n",
    "    for item in wordsAndEmoticons:\n",
    "       # print (item)\n",
    "        v = 0\n",
    "        i = wordsAndEmoticons.index(item)\n",
    "        if (i < len(wordsAndEmoticons)-1 and str(item).lower() == \"kind\" and \\\n",
    "           str(wordsAndEmoticons[i+1]).lower() == \"of\") or str(item).lower() in booster_dict:\n",
    "            sentiments.append(v)\n",
    "            continue\n",
    "        item_lowercase = str(item).lower()\n",
    "        \n",
    "        if  item_lowercase in word_valence_dict:\n",
    "           # print (\"item in valence lexion file--->\",item_lowercase)\n",
    "            v = float(word_valence_dict[item_lowercase])\n",
    "            #print (\"val\", v)\n",
    "            c_incr = 0.733 #(empirically derived mean sentiment intensity rating increase for using ALLCAPs to emphasize a word)\n",
    "            if str(item).isupper() and isCap_diff:\n",
    "                if v > 0: v += c_incr\n",
    "                else: v -= c_incr\n",
    "            def scalar_inc_dec(word, valence):\n",
    "                scalar = 0.0\n",
    "                \n",
    "                word_lower = str(word).lower()\n",
    "                if word_lower in booster_dict:\n",
    "                    #print (\"words in scalar-->\",word_lower)\n",
    "                    scalar = booster_dict[word_lower]\n",
    "                    if valence < 0: scalar *= -1\n",
    "                    if str(word).isupper() and isCap_diff:\n",
    "                        if valence > 0: scalar += c_incr\n",
    "                        else:  scalar -= c_incr\n",
    " #               print (\"scal-->\",scalar) \n",
    "                return scalar\n",
    "            n_scalar = -1.0\n",
    "            if (i+1)<word_len:\n",
    "                if str(wordsAndEmoticons[i+1]).lower() in word_valence_dict and negated([wordsAndEmoticons[i]]) :\n",
    "                    print (\"insde\")\n",
    "\n",
    "            #print (\"previous-->\",wordsAndEmoticons[i-1])                               \n",
    "            try:\n",
    "                if i > 0 and str(wordsAndEmoticons[i-1]).lower() not in word_valence_dict and (i+1)<=word_len:\n",
    "                    s1 = scalar_inc_dec(wordsAndEmoticons[i-1], v)\n",
    "                    v = v+s1\n",
    "                    \n",
    "                    if negated([wordsAndEmoticons[i-1]]):\n",
    "                        \n",
    "                        v = v*n_scalar\n",
    "                        #print (\"insie\",v)\n",
    "                \n",
    "            except:\n",
    "                if i > 0 and str(wordsAndEmoticons[i-1]).lower() not in word_valence_dict and (i+1)<word_len:\n",
    "                    s1 = scalar_inc_dec(wordsAndEmoticons[i-1], v)\n",
    "                    v = v+s1\n",
    "                    \n",
    "                    if negated([wordsAndEmoticons[i-1]]):\n",
    "                        v = v*n_scalar\n",
    "                    #print (\"inside 2\",v)\n",
    "                \n",
    "            if i > 1 and str(wordsAndEmoticons[i-2]).lower() not in word_valence_dict and (i+1)<=word_len:\n",
    "                s2 = scalar_inc_dec(wordsAndEmoticons[i-2], v)\n",
    "                if s2 != 0: s2 = s2*0.95\n",
    "                v = v+s2\n",
    "                if wordsAndEmoticons[i-2] == \"never\" and (wordsAndEmoticons[i-1] == \"so\" or wordsAndEmoticons[i-1] == \"this\"): \n",
    "                    v = v*1.5                    \n",
    "                \n",
    "                elif negated([wordsAndEmoticons[i-2]]):\n",
    "                    v = v*n_scalar\n",
    "            if i > 2 and str(wordsAndEmoticons[i-3]).lower() not in word_valence_dict and (i+1)<word_len:\n",
    "                s3 = scalar_inc_dec(wordsAndEmoticons[i-3], v)\n",
    "                if s3 != 0: s3 = s3*0.9\n",
    "                v = v+s3\n",
    "                if wordsAndEmoticons[i-3] == \"never\" and \\\n",
    "                   (wordsAndEmoticons[i-2] == \"so\" or wordsAndEmoticons[i-2] == \"this\") or \\\n",
    "                   (wordsAndEmoticons[i-1] == \"so\" or wordsAndEmoticons[i-1] == \"this\"):\n",
    "                    v = v*1.25\n",
    "                elif negated([wordsAndEmoticons[i-3]]): v = v*n_scalar\n",
    "                \n",
    "                # check for special case idioms using a sentiment-laden keyword known to SAGE\n",
    "                special_case_idioms = {\"the shit\": 3, \"the bomb\": 3, \"bad ass\": 1.5, \"yeah right\": -2, \n",
    "                                       \"cut the mustard\": 2, \"kiss of death\": -1.5, \"hand to mouth\": -2}\n",
    "                # future work: consider other sentiment-laden idioms\n",
    "                #other_idioms = {\"back handed\": -2, \"blow smoke\": -2, \"blowing smoke\": -2, \"upper hand\": 1, \"break a leg\": 2, \n",
    "                #                \"cooking with gas\": 2, \"in the black\": 2, \"in the red\": -2, \"on the ball\": 2,\"under the weather\": -2}\n",
    "                onezero = \"{} {}\".format(str(wordsAndEmoticons[i-1]), str(wordsAndEmoticons[i]))\n",
    "                twoonezero = \"{} {}\".format(str(wordsAndEmoticons[i-2]), str(wordsAndEmoticons[i-1]), str(wordsAndEmoticons[i]))\n",
    "                twoone = \"{} {}\".format(str(wordsAndEmoticons[i-2]), str(wordsAndEmoticons[i-1]))\n",
    "                threetwoone = \"{} {} {}\".format(str(wordsAndEmoticons[i-3]), str(wordsAndEmoticons[i-2]), str(wordsAndEmoticons[i-1]))\n",
    "                threetwo = \"{} {}\".format(str(wordsAndEmoticons[i-3]), str(wordsAndEmoticons[i-2]))                    \n",
    "                if onezero in special_case_idioms: v = special_case_idioms[onezero]\n",
    "                elif twoonezero in special_case_idioms: v = special_case_idioms[twoonezero]\n",
    "                elif twoone in special_case_idioms: v = special_case_idioms[twoone]\n",
    "                elif threetwoone in special_case_idioms: v = special_case_idioms[threetwoone]\n",
    "                elif threetwo in special_case_idioms: v = special_case_idioms[threetwo]\n",
    "                if len(wordsAndEmoticons)-1 > i:\n",
    "                    zeroone = \"{} {}\".format(str(wordsAndEmoticons[i]), str(wordsAndEmoticons[i+1]))\n",
    "                    if zeroone in special_case_idioms: v = special_case_idioms[zeroone]\n",
    "                if len(wordsAndEmoticons)-1 > i+1:\n",
    "                    zeroonetwo = \"{} {}\".format(str(wordsAndEmoticons[i]), str(wordsAndEmoticons[i+1]), str(wordsAndEmoticons[i+2]))\n",
    "                    if zeroonetwo in special_case_idioms: v = special_case_idioms[zeroonetwo]\n",
    "                \n",
    "                # check for booster/dampener bi-grams such as 'sort of' or 'kind of'\n",
    "                if threetwo in booster_dict or twoone in booster_dict:\n",
    "                    v = v+b_decr\n",
    "            \n",
    "            # check for negation case using \"least\"\n",
    "            if i > 1 and str(wordsAndEmoticons[i-1]).lower() not in word_valence_dict \\\n",
    "                and str(wordsAndEmoticons[i-1]).lower() == \"least\" and (i+1)<word_len:\n",
    "                if (str(wordsAndEmoticons[i-2]).lower() != \"at\" and str(wordsAndEmoticons[i-2]).lower() != \"very\") :\n",
    "                    v = v*n_scalar\n",
    "            elif i > 0 and str(wordsAndEmoticons[i-1]).lower() not in word_valence_dict \\\n",
    "                and str(wordsAndEmoticons[i-1]).lower() == \"least\" and (i+1)<word_len:\n",
    "                v = v*n_scalar\n",
    "       # print (v)\n",
    "        sentiments.append(v) \n",
    "            \n",
    "    # check for modification in sentiment due to contrastive conjunction 'but'\n",
    "    if 'but' in wordsAndEmoticons or 'BUT' in wordsAndEmoticons:#\n",
    "        try: bi = wordsAndEmoticons.index('but')\n",
    "        except: bi = wordsAndEmoticons.index('BUT')\n",
    "        for s in sentiments:\n",
    "            si = sentiments.index(s)\n",
    "            if si <bi :\n",
    "                sentiments.pop(si)\n",
    "                sentiments.insert(si, s*0.8)\n",
    "            elif si>bi :\n",
    "                #print (\"si-->\",si)\n",
    "                sentiments.pop(si)\n",
    "                sentiments.insert(si, s*1.2)\n",
    "    if 'request' in wordsAndEmoticons or 'REQUEST' in wordsAndEmoticons:#\n",
    "        try: bi = wordsAndEmoticons.index('request')\n",
    "        except: bi = wordsAndEmoticons.index('request')\n",
    "        for s in sentiments:\n",
    "            si = sentiments.index(s)\n",
    "            if si <bi :\n",
    "                sentiments.pop(si)\n",
    "                sentiments.insert(si, s*1.8)\n",
    "    \n",
    "               \n",
    " #       print (\"sent-->\",sentiments)\n",
    "                \n",
    "    if sentiments:\n",
    "        \n",
    "        sum_s = float(sum(sentiments))\n",
    "        #print sentiments, sum_s\n",
    "        \n",
    "        # check for added emphasis resulting from exclamation points (up to 4 of them)\n",
    "        ep_count = str(text).count(\"!\")\n",
    "        if ep_count > 4: ep_count = 4\n",
    "        ep_amplifier = ep_count*0.292 #(empirically derived mean sentiment intensity rating increase for exclamation points)\n",
    "        if sum_s > 0:  sum_s += ep_amplifier\n",
    "        elif  sum_s < 0: sum_s -= ep_amplifier\n",
    "        \n",
    "        # check for added emphasis resulting from question marks (2 or 3+)\n",
    "        qm_count = str(text).count(\"?\")\n",
    "        qm_amplifier = 0\n",
    "        if qm_count > 1:\n",
    "            if qm_count <= 3: qm_amplifier = qm_count*0.18\n",
    "            else: qm_amplifier = 0.96\n",
    "            if sum_s > 0:  sum_s += qm_amplifier\n",
    "            elif  sum_s < 0: sum_s -= qm_amplifier\n",
    "       # print (sum_s)\n",
    "        compound = normalize(sum_s)\n",
    "        #print (compound)\n",
    "        # want separate positive versus negative sentiment scores\n",
    "        pos_sum = 0.0\n",
    "        neg_sum = 0.0\n",
    "        neu_count = 0\n",
    "        total = 1\n",
    "        tot = 0\n",
    "        for sentiment_score in sentiments:\n",
    "            if sentiment_score > 0:\n",
    "                pos_sum += (float(sentiment_score) +1) # compensates for neutral words that are counted as 1\n",
    "            if sentiment_score < 0:\n",
    "                neg_sum += (float(sentiment_score) -1) # when used with math.fabs(), compensates for neutrals\n",
    "            if sentiment_score == 0:\n",
    "                neu_count += 1\n",
    "        \n",
    "        if pos_sum > math.fabs(neg_sum): pos_sum += (ep_amplifier+qm_amplifier)\n",
    "        elif pos_sum < math.fabs(neg_sum): neg_sum -= (ep_amplifier+qm_amplifier)\n",
    "\n",
    "        neu_count=normalize(neu_count)\n",
    "        total = (math.fabs(pos_sum) + math.fabs(neg_sum))\n",
    "        if total == 0 :\n",
    "            total = 1\n",
    "#        print pos_sum ,\" \",neg_sum\n",
    "        pos = (math.fabs(pos_sum / total))\n",
    "        neg = (math.fabs(neg_sum / total))\n",
    "        neu = normalize((neu_count/total))\n",
    "    else:\n",
    "        compound = 0.0; pos = 0.0; neg = 0.0; neu = 0.0\n",
    "#    print neg\n",
    "    tot = pos+(-1)*neg+neu\n",
    "\n",
    "        \n",
    "    s = {\"neg\" : round(neg, 3), \n",
    "         \"neu\" : round(neu, 3),\n",
    "         \"pos\" : round(pos, 3),\n",
    "         \"compound\" : round(compound, 4),\n",
    "         \"total\" : round(tot,3)}\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def vader(x):\n",
    "    #print type(str(x))\n",
    "   # row = list(x)\n",
    "    try:\n",
    "        x=x.encode('utf-8')\n",
    "        sentence = str(x).encode('ascii','ignore').decode('ascii')\n",
    "        i=0\n",
    "        temp_p=0\n",
    "        temp_n=0\n",
    "        temp_c=0\n",
    "        lines_list = re.split(r'[.!?]+', str(sentence))\n",
    "        for sent in lines_list:\n",
    "            sent=sent.strip().lower()\n",
    "            if sent == '':\n",
    "                print (\"empty\")\n",
    "                continue\n",
    "            else:\n",
    "                #print sent\n",
    "                ss = sentiment(sent)\n",
    "                #print (ss)\n",
    "                if not all(value == 0 for value in ss.values()):\n",
    "                    i=i+1\n",
    "                temp_p=temp_p+ss['pos']\n",
    "                temp_n=temp_n+ss['neg']\n",
    "                temp_c=temp_c+ss['compound']\n",
    "        #print (\"i -->\",i)\n",
    "       # print temp_c\n",
    "        #print i\n",
    "        if i==0:\n",
    "            i=1\n",
    "        if len(str(sentence).split(\" \")) <4:\n",
    "            i=i*2\n",
    "        tc=float(temp_c/float(i))*100\n",
    "        tp=float(temp_p/float(i))*100\n",
    "        tn = float(temp_n/float(i))*100\n",
    "    except:\n",
    "        tc = 0\n",
    "    return tc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_file['confidence_score'] = review_file.review_text.apply(lambda row: vader(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from gensim.utils import to_unicode, smart_open, dict_from_corpus\n",
    "import pandas as pd\n",
    "import logging\n",
    "import argparse\n",
    "from collections import namedtuple\n",
    "from gensim.utils import smart_open, to_utf8, tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n",
    "import random\n",
    "\n",
    "def dict_of_phrases(phrase_dir):\n",
    "    phrase_dict = dict()\n",
    "    global_dict = dict()\n",
    "\n",
    "\n",
    "    # Making a dict() for all the phrases we have\n",
    "    # and ensure that the phrases don't have underscores in them\n",
    "\n",
    "    for DIR in os.listdir(phrase_dir):\n",
    "        phrase_dict[DIR] = dict()\n",
    "        if DIR.startswith(\".\"):\n",
    "            continue\n",
    "        for infile in os.listdir(os.path.join(phrase_dir, DIR)):\n",
    "            if infile.startswith(\".\"):\n",
    "                continue\n",
    "            if infile.endswith(\".txt\") :\n",
    "                fname = infile.split(\".txt\")[0]\n",
    "                phrase_dict[DIR][fname] = []\n",
    "\n",
    "                # reading the phrases in file\n",
    "                with open(phrase_dir + \"/\" + DIR + \"/\" + infile, 'rb') as foo:\n",
    "\n",
    "\n",
    "\n",
    "                    line_num = 0\n",
    "                    for line in foo:\n",
    "                        if \"\\r\" in line:\n",
    "                            linee = line.split(\"\\r\")\n",
    "                            for line in linee:\n",
    "                                line_num += 1\n",
    "\n",
    "                                # Replacing underscores with spaces\n",
    "                                line = line.decode('ascii', 'ignore').replace(\"_\", \" \").replace(\"\\n\\r\", \"\\n\")\n",
    "\n",
    "                                # Replacing dashes with spaces so as to prevent errors\n",
    "                                # in detecting word boundaries\n",
    "                                line = line.replace(\"-\", \" \")\n",
    "                                phrase = line.strip().split(\"|\")[0]\n",
    "\n",
    "                                # check for empty line\n",
    "                                if phrase == \"\":\n",
    "                                    continue\n",
    "\n",
    "                                if phrase not in phrase_dict:\n",
    "                                    phrase_dict[DIR][fname].append(phrase)\n",
    "                                    global_dict[phrase] \\\n",
    "                                        = 1\n",
    "                        else:\n",
    "                            line_num += 1\n",
    "\n",
    "                            # Replacing underscores with spaces\n",
    "                            line = line.decode('ascii','ignore').replace(\"_\", \" \").replace(\"\\n\\r\",\"\\n\")\n",
    "\n",
    "                            # Replacing dashes with spaces so as to prevent errors\n",
    "                            # in detecting word boundaries\n",
    "                            line = line.replace(\"-\", \" \")\n",
    "                            phrase = line.strip().split(\"|\")[0]\n",
    "\n",
    "                            # check for empty line\n",
    "                            if phrase == \"\":\n",
    "                                #print line_num\n",
    "                                continue\n",
    "                                \n",
    "                            if phrase not in phrase_dict:\n",
    "                                phrase_dict[DIR][fname].append(phrase)\n",
    "\n",
    "\n",
    "                                global_dict[phrase] = 1\n",
    "                                    \n",
    "    return phrase_dict, global_dict\n",
    "\n",
    "def load_negation_words(negation_words_file):\n",
    "    \"\"\" reads negation words from the \"negation.sorted.txt file\"\n",
    "    :param negation_words_file: file containing words which induce negation\n",
    "    :return: list of negation words\n",
    "    \"\"\"\n",
    "    #neg_words_file = os.path.join(negation_words_dir, \"negation.sorted.txt\")\n",
    "\n",
    "    # reading the entire file using readlines()\n",
    "    neg_words_list = smart_open(negation_words_file, mode='rb').readlines()\n",
    "\n",
    "    # striping the \"/n\" at the end of every line\n",
    "    neg_words_list = list(word.strip() for word in neg_words_list)\n",
    "    return neg_words_list\n",
    "\n",
    "def check_super_phrase(phrase_list, phrase_dict, window=16):\n",
    "    \"\"\"the phrases are in lowercase,\n",
    "    We will iterate through phrases in the phrase_list and see\n",
    "    if there is any sub-phrase in the phrase_dict after deleting\n",
    "    the current phrase .It will store the new super phrases\n",
    "    We will remove sub phrases if they are found\n",
    "\n",
    "    :param phrase_list: list of valid phrases found in line\n",
    "    :param phrase_dict: copy of the above list in dict form\n",
    "    :param window: size of window to be considered\n",
    "    :return: list of valid super phrases\n",
    "    \"\"\"\n",
    "\n",
    "    new_phrase_dict = phrase_dict.copy()\n",
    "\n",
    "    for num_phr, phrase in enumerate(phrase_list):\n",
    "\n",
    "        # flag variable checks for the presence of sub-phrase\n",
    "        temp_phrase_dict = phrase_dict.copy()\n",
    "\n",
    "        # deletes the current phrase from the dictionary\n",
    "        del temp_phrase_dict[phrase]\n",
    "\n",
    "        # tokenizes the current phrase with \"whitespace\" separator\n",
    "        toks = phrase.split(\" \")\n",
    "\n",
    "        # Iterates through the toks to check for presence of sub-phrase\n",
    "        # Here we will have to iterate through the toks[0:] since we also\n",
    "        # need to find unigrams\n",
    "        for i, start_token in enumerate(toks[0:]):\n",
    "\n",
    "            # Since we will be having uni-grams in curated phrases,\n",
    "            # the arguments to range function will be given as range(0, window)\n",
    "            for j in range(0, window):\n",
    "                if i + j + 1 <= len(toks):\n",
    "                    new_string = \" \".join(toks[i:i + j + 1])\n",
    "                    if new_string in temp_phrase_dict:\n",
    "                        if new_string in new_phrase_dict:\n",
    "                            del new_phrase_dict[new_string]\n",
    "\n",
    "    return new_phrase_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "phrase_dir = path+\"/Pedigree_autoresponses/fine_grained_phrases_pedigree_sentiment_sub_clusters_old/\"\n",
    "replies_dir = path+\"/Pedigree_autoresponses/responses_sub_clusters/\"\n",
    "negation_words_file = \"/Users/devanshg/Desktop/enixta-machine-learning-master/Smaartpulse_Python_Base/data_annotation/negation_words.txt\"\n",
    "phrase_dict, global_dict = dict_of_phrases(phrase_dir=phrase_dir)\n",
    "del phrase_dict['.DS_Store']\n",
    "#negation_words_list = load_negation_words(negation_words_file= negation_words_file)\n",
    "replies_dict, global_replies_dict = dict_of_phrases(replies_dir)\n",
    "questions_file = open( \"Pedigree_autoresponses/hot_warm_dont_reply_classification/questions_words.txt\", 'r')\n",
    "next_4 = questions_file.readline()\n",
    "\n",
    "questions_list = []\n",
    "while next_4 != \"\": \n",
    "    questions_list.append(next_4.strip())\n",
    "    next_4 = questions_file.readline()\n",
    "\n",
    "#reliability_list = reliability_list[0].split('\\r')\n",
    "question_words = []\n",
    "for z in questions_list:\n",
    "    z_list = z.split('\\r')\n",
    "    question_words = question_words + z_list\n",
    "\n",
    "aspects_location = path+'/Pedigree_autoresponses/hot_warm_dont_reply_classification/'\n",
    "\n",
    "dont_reply_file = open(aspects_location + \"dont-reply.txt\", 'r')\n",
    "next_2 = dont_reply_file.readline()\n",
    "\n",
    "dont_reply_list = []\n",
    "while next_2 != \"\": \n",
    "    dont_reply_list.append(next_2.strip())\n",
    "    next_2 = dont_reply_file.readline()\n",
    "\n",
    "dont_reply_clusters =[words for segments in dont_reply_list for words in segments.replace('\\n','').split('\\r')]\n",
    "dont_reply_clusters = filter(None, dont_reply_clusters)\n",
    "dont_reply_aspects = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob('Pedigree_autoresponses/fine_grained_phrases_pedigree_sentiment_sub_clusters_old/aspects/*'):\n",
    "    t =file.replace('Pedigree_autoresponses/fine_grained_phrases_pedigree_sentiment_sub_clusters_old/aspects/','')\n",
    "    t = t.replace('.txt','')\n",
    "    if t in dont_reply_clusters:\n",
    "        f = open(file,'r+')\n",
    "        dont_reply_aspects.append(f.readlines())\n",
    "dont_reply_aspects=list(chain.from_iterable(dont_reply_aspects))\n",
    "dont_reply_aspects=[i.replace('\\n','').split('\\r', 1)[0] for i in dont_reply_aspects]\n",
    "dont_reply_aspects= filter(None, dont_reply_aspects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def get_ngrams(text ):\n",
    "    list1 =[]\n",
    "    for i in range(1,len(text.split(' '))):\n",
    "        n_grams = ngrams(word_tokenize(text), i)\n",
    "        list1.append([ ' '.join(grams) for grams in n_grams])\n",
    "    list1=list(chain.from_iterable(list1))\n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def aspect_finder():\n",
    "aspect_out =[]\n",
    "for row in review_file.to_dict('records'):\n",
    "    rcs = 100.0\n",
    "    dead = ['died','death','dying','hospit','killed','ill']\n",
    "    aspect = []\n",
    "    aspect_keyword = []\n",
    "    flag =0\n",
    "    #try:\n",
    "    temp_text = row['review_text'].encode('ascii','ignore')\n",
    "    text = str(temp_text).lower()\n",
    "    txt =get_ngrams(text)\n",
    "    ques = set(txt)&set(question_words) \n",
    "    ques = sorted(ques, key = lambda k : txt.index(k))\n",
    "    if any(ques):\n",
    "        flag =1\n",
    "        math.ceil(random.uniform(78.4, 85.9)*100)/100\n",
    "    die = set(txt)&set(dead) \n",
    "    die = sorted(die, key = lambda k : txt.index(k))\n",
    "    if any(die):\n",
    "        flag =2\n",
    "        rcs = math.ceil(random.uniform(95.4, 99.9)*100)/100\n",
    "        \n",
    "\n",
    "    for file in glob.glob('Pedigree_autoresponses/fine_grained_phrases_pedigree_sentiment_sub_clusters_old/aspects/*'):\n",
    "        t =file.replace('Pedigree_autoresponses/fine_grained_phrases_pedigree_sentiment_sub_clusters_old/aspects/','')\n",
    "        t = t.replace('.txt','')\n",
    "        f = open(file,'r+')\n",
    "        keyword = f.readlines()\n",
    "        keyword =[i.replace('\\n','').split('\\r', 1)[0] for i in keyword]\n",
    "        list1 = set(keyword)&set(txt) \n",
    "        list2 = sorted(list1, key = lambda k : txt.index(k))\n",
    "        #if re.search(r'\\bgood\\b','A good product, loved by my pet.'):\n",
    "            #print('correct')\n",
    "        if any(list2) :\n",
    "            aspect.append(t)\n",
    "            aspect_keyword.append(list2)\n",
    "    aspect_keyword=list(chain.from_iterable(aspect_keyword))\n",
    "    aspect_keyword=[i.replace('\\n','').split('\\r', 1)[0] for i in aspect_keyword]\n",
    "    match = set(aspect_keyword)&set(dont_reply_aspects) \n",
    "    match = sorted(match, key = lambda k : aspect_keyword.index(k))\n",
    "    print match\n",
    "    if any(match):\n",
    "        rcs = float(rcs - float(len(match)*.65))\n",
    "        if len(match) == len(aspect_keyword):\n",
    "            aspect.append('dont_reply')\n",
    "            flag = 3\n",
    "            rcs = 0.0\n",
    "#     except:\n",
    "#         aspect =['Unicode error']\n",
    "#         aspect_keyword =['error']\n",
    "        #return aspect,aspect_keyword\n",
    "    if len(temp_text) < 80 and len(aspect_keyword) < 1:\n",
    "        #aspects_missed = expected - actual\n",
    "        aspects_missed = 1 - len(aspect_keyword)\n",
    "    elif len(temp_text) >= 80 and len(temp_text)< 180 and len(aspect_keyword) < 2:\n",
    "        aspects_missed = 2 - len(aspect_keyword)\n",
    "    elif len(temp_text) >= 180 and len(temp_text) < 450 and len(aspect_keyword) < 3:\n",
    "        aspects_missed = 3 - len(aspect_keyword)\n",
    "    elif len(temp_text) >= 450 and len(aspect_keyword) < 4:\n",
    "        aspects_missed = 4 - len(aspect_keyword)\n",
    "    else:\n",
    "        aspects_missed = 0\n",
    "    rcs = rcs - 4*aspects_missed*.5\n",
    "    if row['reviewer_name'] == 'Pedigree Expert':\n",
    "        rcs=0\n",
    "    row['aspect']=aspect\n",
    "    row['aspect_keyword'] = aspect_keyword\n",
    "    row['flag'] = flag\n",
    "    score = row['confidence_score']\n",
    "    if int(score) <= 0:\n",
    "        row['polarity'] = 'negative'\n",
    "    if int(score) > 0:\n",
    "        row['polarity'] = 'positive'\n",
    "    if int(flag) == 1:\n",
    "        row['polarity'] = 'neutral'\n",
    "    row['response_confidence_score'] = rcs\n",
    "    aspect_out.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-43-955f8976831b>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-43-955f8976831b>\"\u001b[0;36m, line \u001b[0;32m23\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def replier(x):\n",
    "    row = dict(x)\n",
    "    replies_hot = replies_dict[replies_dict.aspect == 'hot-clusters']\n",
    "    replies_overall = replies_dict[replies_dict.aspect == 'product-overall-5stars']\n",
    "    conf_score = row['confidence_score']\n",
    "    review_text = row['review_text']\n",
    "    try:\n",
    "        reviewer_name = str(row['reviewer_name'])\n",
    "        print reviewer_name\n",
    "        reviewer_name = str(reviewer_name).encode('utf-8').encode('ascii','ignore').decode('ascii')\n",
    "    except:\n",
    "        reviewer_name = 'Customer'\n",
    "    star_rating = row['star_rating']\n",
    "    flag = row['flag']\n",
    "    if conf_score < 0:\n",
    "        if flag == 0 or flag =='3':\n",
    "            reply = \"Dear \"+ str(reviewer_name)\n",
    "            reply = reply +', '+random.choice(list(replies_hot.template_response))\n",
    "        if flag == 2:\n",
    "            reply = \"Dear \"+reviewer_name+\", We are deeply saddened to hear about your dog. We request you to share your contact details so that we can assist you ASAP at our toll free number 1800-4071-12121 or write to us at pedigree.india@effem.com. Regards Team Pedigree.\"\n",
    "    if conf_score == 0:\n",
    "        \n",
    "    else:\n",
    "        reply = reply = reply +', '+random.choice(list(replies_overall.template_response))\n",
    "        if \"customer name\" in reply:\n",
    "            reply=reply.replace('customer name',str(reviewer_name))\n",
    "        else:\n",
    "            x = \"Dear \"+ str(reviewer_name)\n",
    "            reply = x +', '+reply\n",
    "    if flag == 1:\n",
    "        reply = \"Dear \"+reviewer_name+\", for any queries and facts regarding your dog/cat please refer to the FAQs att our official site https://www.pedigree.com/faqs. Please feel free to reach us at our toll free number 1800-4071-12121 or write to us at pedigree.india@effem.com for any further assistance. Regards Team Pedigree.\"\n",
    "    if row['reviewer_name'] == 'Pedigree Expert':\n",
    "        reply = \"\"\n",
    "    \n",
    "\n",
    "    return reply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = aspect_out[['aspect','id','source_review_id','polarity']]\n",
    "# df = aspect_out[aspect_out.source_review_id == 'R1BPUSEVELFI83']\n",
    "# df['response'] = df.apply(lambda row: replier(row), axis = 1)\n",
    "# list(df.response.str.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_resp = aspect_out\n",
    "aspect_out['response_2'] = \"\"\n",
    "aspect_out['response_3'] = \"\"\n",
    "aspect_out['degree'] = \"\"\n",
    "# del final_resp['aspect']\n",
    "# del final_resp['aspect_keyword']\n",
    "# del final_resp['flag']\n",
    "final_resp.id = final_resp.id.astype(int)\n",
    "cols = ['id','source_review_id','response','response_2','response_3','confidence_score','polarity','degree','response_confidence_score']\n",
    "\n",
    "final_resp = final_resp[cols]\n",
    "final_resp.to_csv('responses_mars_pedigree_10th_may_2018_final_old_aspects.txt',sep='~',quoting=csv.QUOTE_ALL,index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#aspect_out[aspect_out.review_text.str.contains('expir')\n",
    "insight = pd.read_csv('responses_mars_pedigree_25th_april_2018_all_headers.txt',sep='~',quoting=csv.QUOTE_ALL,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_breed = insight[insight.aspect.str.contains('dog-user')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_breed['aspect']=(dog_breed['aspect']).astype(str).str.replace('[','').astype(str).str.replace(']','').astype(str).str.split(',')\n",
    "dog_breed['aspect_keyword']=(dog_breed['aspect_keyword']).astype(str).str.replace('[','').astype(str).str.replace(']','').astype(str).str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['convienience', ' dog-user-breed'],\n",
       " ['all',\n",
       "  ' condition_of_product',\n",
       "  ' convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' smell'],\n",
       " ['convienience', ' delivery', ' dog-user-breed', ' pricing'],\n",
       " ['dog-user-breed', ' information', ' pricing'],\n",
       " ['all',\n",
       "  ' convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' health_impact',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' pricing',\n",
       "  ' taste'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed', ' information', ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['all',\n",
       "  ' dog-user-breed',\n",
       "  ' information',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' taste'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['convienience', ' dog-user-breed'],\n",
       " ['dog-user-breed', ' nutrition_ingredientst', ' sizing', ' smell'],\n",
       " ['all', ' dog-user-breed', ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' information', ' nutrition_ingredientst'],\n",
       " ['all', ' condition_of_product', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['convienience', ' dog-user-breed', ' information'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' convienience', ' dog-user-breed'],\n",
       " ['dog-user-breed', ' nutrition_ingredientst', ' pricing'],\n",
       " ['condition_of_product', ' dog-user-breed'],\n",
       " ['all',\n",
       "  ' convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' information',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' pricing',\n",
       "  ' sizing'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' offers_n_discounts', ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' convienience', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed', ' pricing'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all',\n",
       "  ' condition_of_product',\n",
       "  ' convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' information',\n",
       "  ' pricing'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' delivery', ' dog-user-breed', ' information', ' pricing', ' taste'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['all', ' dog-user-breed', ' taste'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all',\n",
       "  ' condition_of_product',\n",
       "  ' convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' pricing',\n",
       "  ' sizing',\n",
       "  ' smell',\n",
       "  ' taste'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed', ' pricing'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' shelf_life'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['all',\n",
       "  ' delivery',\n",
       "  ' dog-user-breed',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' offers_n_discounts',\n",
       "  ' pricing',\n",
       "  ' sizing'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed', ' taste'],\n",
       " ['all',\n",
       "  ' dog-user-breed',\n",
       "  ' information',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' pricing'],\n",
       " ['convienience', ' dog-user-breed', ' information'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' health_impact'],\n",
       " ['dog-user-breed', ' health_impact', ' smell'],\n",
       " ['dog-user-breed', ' smell'],\n",
       " ['all', ' condition_of_product', ' convienience', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst', ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst', ' pricing'],\n",
       " ['all',\n",
       "  ' delivery',\n",
       "  ' dog-user-breed',\n",
       "  ' health_impact',\n",
       "  ' information',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' smell',\n",
       "  ' taste'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' offers_n_discounts', ' shelf_life'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' convienience', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' convienience', ' dog-user-breed', ' information', ' pricing'],\n",
       " ['dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all',\n",
       "  ' convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' health_impact',\n",
       "  ' nutrition_ingredientst'],\n",
       " ['all', ' dog-user-breed', ' health_impact', ' pricing'],\n",
       " ['all', ' convienience', ' dog-user-breed'],\n",
       " ['convienience', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed', ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst', ' taste'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst', ' pricing'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' delivery', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' condition_of_product', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed', ' health_impact'],\n",
       " ['all', ' dog-user-breed', ' health_impact'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['condition_of_product',\n",
       "  ' convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' nutrition_ingredientst'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' pricing'],\n",
       " ['all', ' delivery', ' dog-user-breed', ' information', ' smell'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all',\n",
       "  ' delivery',\n",
       "  ' dog-user-breed',\n",
       "  ' health_impact',\n",
       "  ' nutrition_ingredientst'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['condition_of_product', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' appearance', ' dog-user-breed', ' health_impact'],\n",
       " ['dog-user-breed'],\n",
       " ['convienience', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' sizing'],\n",
       " ['all', ' convienience', ' dog-user-breed', ' pricing'],\n",
       " ['dog-user-breed', ' smell'],\n",
       " ['condition_of_product',\n",
       "  ' convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' information',\n",
       "  ' pricing',\n",
       "  ' sizing',\n",
       "  ' taste'],\n",
       " ['all', ' convienience', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['convienience', ' delivery', ' dog-user-breed', ' pricing'],\n",
       " ['convienience', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all',\n",
       "  ' convienience',\n",
       "  ' delivery',\n",
       "  ' dog-user-breed',\n",
       "  ' information',\n",
       "  ' shelf_life'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' convienience', ' dog-user-breed'],\n",
       " ['all', ' convienience', ' dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' taste'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst', ' sizing'],\n",
       " ['convienience', ' dog-user-breed'],\n",
       " ['dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['dog-user-breed', ' information'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' offers_n_discounts'],\n",
       " ['dog-user-breed', ' smell'],\n",
       " ['all', ' convienience', ' dog-user-breed', ' pricing'],\n",
       " ['all', ' dog-user-breed', ' health_impact'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst', ' pricing'],\n",
       " ['all', ' dog-user-breed', ' taste'],\n",
       " ['convienience', ' dog-user-breed', ' nutrition_ingredientst', ' sizing'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' convienience', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all',\n",
       "  ' convienience',\n",
       "  ' delivery',\n",
       "  ' dog-user-breed',\n",
       "  ' information',\n",
       "  ' offers_n_discounts',\n",
       "  ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' convienience', ' dog-user-breed'],\n",
       " ['all',\n",
       "  ' dog-user-breed',\n",
       "  ' health_impact',\n",
       "  ' information',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' smell',\n",
       "  ' taste'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' convienience', ' dog-user-breed', ' pricing'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed', ' sizing', ' smell'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all',\n",
       "  ' convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' pricing'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' health_impact'],\n",
       " ['dog-user-breed'],\n",
       " ['all',\n",
       "  ' condition_of_product',\n",
       "  ' dog-user-breed',\n",
       "  ' nutrition_ingredientst'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed', ' health_impact', ' information', ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' convienience', ' dog-user-breed', ' shelf_life', ' sizing'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all',\n",
       "  ' condition_of_product',\n",
       "  ' convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' health_impact',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' pricing',\n",
       "  ' taste'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' convienience', ' dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all',\n",
       "  ' dog-user-breed',\n",
       "  ' information',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' health_impact'],\n",
       " ['all', ' convienience', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst', ' taste'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['all',\n",
       "  ' dog-user-breed',\n",
       "  ' health_impact',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' pricing'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['all', ' delivery', ' dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['appearance', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' pricing'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' condition_of_product', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['condition_of_product', ' dog-user-breed'],\n",
       " ['convienience', ' dog-user-breed', ' offers_n_discounts', ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' delivery', ' dog-user-breed', ' pricing'],\n",
       " ['all', ' dog-user-breed', ' pricing'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all',\n",
       "  ' condition_of_product',\n",
       "  ' convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' information',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' pricing'],\n",
       " ['dog-user-breed', ' health_impact'],\n",
       " ['all',\n",
       "  ' convienience',\n",
       "  ' delivery',\n",
       "  ' dog-user-breed',\n",
       "  ' pricing',\n",
       "  ' sizing',\n",
       "  ' smell'],\n",
       " ['condition_of_product', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' convienience', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed', ' health_impact'],\n",
       " ['all',\n",
       "  ' convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' health_impact',\n",
       "  ' information',\n",
       "  ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['convienience', ' dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' information',\n",
       "  ' offers_n_discounts',\n",
       "  ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' pricing'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed', ' health_impact', ' smell', ' taste'],\n",
       " ['dog-user-breed', ' health_impact'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' convienience', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' appearance', ' dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all',\n",
       "  ' convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' health_impact',\n",
       "  ' information',\n",
       "  ' pricing'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' delivery', ' dog-user-breed', ' pricing'],\n",
       " ['appearance', ' convienience', ' dog-user-breed'],\n",
       " ['all', ' delivery', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['delivery', ' dog-user-breed'],\n",
       " ['all', ' condition_of_product', ' convienience', ' dog-user-breed'],\n",
       " ['convienience', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' health_impact', ' pricing'],\n",
       " ['convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' health_impact',\n",
       "  ' nutrition_ingredientst'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' information'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst', ' smell'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' delivery', ' dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['all', ' dog-user-breed', ' offers_n_discounts', ' pricing'],\n",
       " ['all', ' dog-user-breed', ' health_impact', ' nutrition_ingredientst'],\n",
       " ['dog-user-breed', ' pricing'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' condition_of_product', ' dog-user-breed', ' pricing'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all',\n",
       "  ' convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' information',\n",
       "  ' pricing',\n",
       "  ' taste'],\n",
       " ['dog-user-breed', ' nutrition_ingredientst', ' taste'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['convienience', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' appearance', ' convienience', ' dog-user-breed'],\n",
       " ['dog-user-breed', ' smell'],\n",
       " ['dog-user-breed'],\n",
       " ['convienience', ' dog-user-breed'],\n",
       " ['all',\n",
       "  ' convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' sizing'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' convienience', ' delivery', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' health_impact'],\n",
       " ['all', ' convienience', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['all', ' dog-user-breed', ' taste'],\n",
       " ['convienience', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' delivery', ' dog-user-breed', ' taste'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed', ' sizing'],\n",
       " ['all', ' dog-user-breed', ' health_impact'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' pricing'],\n",
       " ['all', ' dog-user-breed', ' information'],\n",
       " ['all', ' convienience', ' delivery', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed', ' smell'],\n",
       " ['dog-user-breed', ' sizing'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst', ' sizing'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' pricing'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' health_impact'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['delivery', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['appearance', ' convienience', ' dog-user-breed', ' pricing'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['convienience', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst', ' pricing'],\n",
       " ['dog-user-breed', ' nutrition_ingredientst', ' taste'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' convienience', ' dog-user-breed', ' health_impact'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' health_impact', ' pricing', ' smell'],\n",
       " ['all', ' dog-user-breed', ' pricing'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' delivery', ' dog-user-breed'],\n",
       " ['delivery', ' dog-user-breed', ' pricing'],\n",
       " ['all', ' dog-user-breed', ' health_impact'],\n",
       " ['all', ' convienience', ' dog-user-breed', ' pricing'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' delivery', ' dog-user-breed'],\n",
       " ['all', ' convienience', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed', ' health_impact', ' pricing'],\n",
       " ['all',\n",
       "  ' condition_of_product',\n",
       "  ' dog-user-breed',\n",
       "  ' nutrition_ingredientst'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed', ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' health_impact'],\n",
       " ['condition_of_product',\n",
       "  ' convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' sizing'],\n",
       " ['dog-user-breed', ' smell'],\n",
       " ['dog-user-breed',\n",
       "  ' health_impact',\n",
       "  ' information',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['convienience', ' dog-user-breed', ' pricing'],\n",
       " ['dog-user-breed', ' nutrition_ingredientst', ' taste'],\n",
       " ['dog-user-breed'],\n",
       " ['all',\n",
       "  ' convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' health_impact',\n",
       "  ' information',\n",
       "  ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed', ' information', ' nutrition_ingredientst', ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['convienience', ' dog-user-breed', ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed', ' pricing', ' taste'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['all',\n",
       "  ' condition_of_product',\n",
       "  ' dog-user-breed',\n",
       "  ' nutrition_ingredientst'],\n",
       " ['dog-user-breed'],\n",
       " ['all',\n",
       "  ' condition_of_product',\n",
       "  ' convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' information',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' offers_n_discounts',\n",
       "  ' pricing'],\n",
       " ['dog-user-breed', ' health_impact', ' nutrition_ingredientst'],\n",
       " ['all', ' dog-user-breed', ' health_impact'],\n",
       " ['dog-user-breed', ' health_impact'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['convienience', ' dog-user-breed', ' pricing', ' shelf_life', ' taste'],\n",
       " ['convienience', ' dog-user-breed', ' pricing'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' appearance', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' health_impact'],\n",
       " ['dog-user-breed', ' pricing'],\n",
       " ['convienience', ' dog-user-breed', ' taste'],\n",
       " ['all', ' delivery', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['condition_of_product', ' dog-user-breed'],\n",
       " ['convienience',\n",
       "  ' dog-user-breed',\n",
       "  ' information',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' pricing'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' condition_of_product', ' convienience', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' information', ' pricing'],\n",
       " ['dog-user-breed'],\n",
       " ['convienience', ' dog-user-breed', ' health_impact'],\n",
       " ['appearance', ' dog-user-breed'],\n",
       " ['all', ' appearance', ' dog-user-breed'],\n",
       " ['dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed', ' health_impact'],\n",
       " ['dog-user-breed'],\n",
       " ['all',\n",
       "  ' condition_of_product',\n",
       "  ' dog-user-breed',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' smell'],\n",
       " ['dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' health_impact'],\n",
       " ['all', ' convienience', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['convienience', ' dog-user-breed'],\n",
       " ['dog-user-breed', ' health_impact'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed', ' sizing'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed', ' pricing'],\n",
       " ['all', ' dog-user-breed', ' pricing'],\n",
       " ['dog-user-breed', ' pricing'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all',\n",
       "  ' condition_of_product',\n",
       "  ' dog-user-breed',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' taste'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['convienience', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed', ' taste'],\n",
       " ['all', ' delivery', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['dog-user-breed'],\n",
       " ['delivery', ' dog-user-breed', ' pricing'],\n",
       " ['condition_of_product', ' dog-user-breed', ' offers_n_discounts'],\n",
       " ['all',\n",
       "  ' condition_of_product',\n",
       "  ' dog-user-breed',\n",
       "  ' nutrition_ingredientst',\n",
       "  ' sizing'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['convienience', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed', ' taste'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed', ' health_impact'],\n",
       " ['all', ' delivery', ' dog-user-breed'],\n",
       " ['condition_of_product', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' delivery', ' dog-user-breed', ' offers_n_discounts'],\n",
       " ['dog-user-breed'],\n",
       " ['convienience', ' delivery', ' dog-user-breed', ' offers_n_discounts'],\n",
       " ['all', ' delivery', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' condition_of_product', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' delivery', ' dog-user-breed', ' shelf_life'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' convienience', ' dog-user-breed', ' information'],\n",
       " ['all', ' dog-user-breed', ' offers_n_discounts'],\n",
       " ['dog-user-breed'],\n",
       " ['convienience', ' dog-user-breed', ' nutrition_ingredientst'],\n",
       " ['dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['all', ' dog-user-breed'],\n",
       " ['dog-user-breed'],\n",
       " ['dog-user-breed', ' pricing']]"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dog_breed['aspect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "out =[]\n",
    "f = open('Pedigree_autoresponses/fine_grained_phrases_pedigree_sentiment_sub_clusters/aspects/dog-user-breed.txt','r+')\n",
    "breed_names = [i.replace('\\n','') for i in list(f.readlines())]\n",
    "np_array = np.array(breed_names)\n",
    "for row in dog_breed.to_dict('records'):\n",
    "  #  a = list(row['aspect'])\n",
    "    #a = [x.strip(' ') for x in a]\n",
    "    #index = a.index('dog-user-breed')\n",
    "    key = list(row['aspect_keyword'])\n",
    "    key = [x.strip(' ') for x in key]\n",
    "    item_index =np.in1d(key,np_array).nonzero()[0]\n",
    "    print item_index\n",
    "    for i in range(0,len(item_index)):\n",
    "        row['dog_breed_name'] = key[item_index[i]]\n",
    "        out.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filter = out[out.dog_breed_name.isin(breed_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filter_1=[]\n",
    "for row in out_filter.to_dict(\"records\"):\n",
    "    row['dog_breed_name'] = row['dog_breed_name'].replace('gsd',\"german shepherd\").replace('cocker',\"cocker spaniel\").replace('cocker spaniel spaniel',\"cocker spaniel\").replace('shepard',\"german shepherd\").replace('shepherd',\"german shepherd\").replace('german german shepherd',\"german shepherd\").replace('german shepherds',\"german shepherd\").replace('shepherd',\"german shepherd\").replace('german german shepherd',\"german shepherd\").replace('retriver',\"retriever\").replace('bull',\"bull dog\").replace('labs',\"labrador\").replace('bull dogdog',\"bull dog\").lower()\n",
    "    out_filter_1.append(row)\n",
    "out_filter_1 = pd.DataFrame(out_filter_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filter_1.to_csv('Breed_Wise_and_review_wise_polarity_insights.csv',sep=',',quoting=csv.QUOTE_ALL,encoding='utf-8',index=False,columns =['id','source_review_id','dog_breed_name','polarity','confidence_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['german shepherd', 'labrador', 'pug', 'dachshund', 'bernard',\n",
       "       'rottweiler', 'dane', 'bull dog', 'boxer', 'retriever', 'beagle',\n",
       "       'part', 'cocker spaniel', 'brand dog', 'tzu', 'crack', 'terrier',\n",
       "       'husky', 'mastiff', 'mutt', 'furbaby', 'bosco'], dtype=object)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_filter_1.dog_breed_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_t_per=[]\n",
    "breed = out_filter_1.dog_breed_name.unique()\n",
    "#print map_comp_tree\n",
    "for c in breed:\n",
    "    comp_rev = out_filter_1[out_filter_1.dog_breed_name == c]\n",
    "    date_df = comp_rev [comp_rev.polarity == 'positive']\n",
    "    #row = [c,date_df.shape[0],comp_rev.shape[0],float(date_df.shape[0]/float(comp_rev.shape[0]))*100,float((comp_rev.shape[0]-date_df.shape[0])/float(comp_rev.shape[0]))*100]\n",
    "    row\n",
    "    out_t_per.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_t_per=pd.DataFrame(out_t_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_t_per.columns=['Breed Name','Positive review count','Total count','Positive percentage','Negative percentage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_t_per.to_csv('Breed_Wise_polarity_insights.csv',sep=',',quoting=csv.QUOTE_ALL,encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(341, 15)"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_filter_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1=[]\n",
    "for row in df.to_dict('records'):\n",
    "    key = list(row['aspect'])\n",
    "    key = [x.strip(' ') for x in key]\n",
    "    for i in range(0,len(key)):\n",
    "        print key[i]\n",
    "        if key[i] != 'dont_reply':\n",
    "            row1 = [row['id'],row['source_review_id'],key[i],row['polarity']]\n",
    "            out1.append(row1)\n",
    "out1=pd.DataFrame(out1)\n",
    "out1.columns= ['id','source_review_id','aspect_id','polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1.to_csv('aspect_id_map.csv',sep=',',quoting=csv.QUOTE_ALL,encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import unicodedata\n",
    "import glob\n",
    "import argparse\n",
    "import math, re, sys, fnmatch, string\n",
    "import pandas as pd\n",
    "import unicodecsv as csv\n",
    "import enchant\n",
    "import os\n",
    "from gensim.utils import to_unicode, smart_open, dict_from_corpus\n",
    "import pandas as pd\n",
    "import logging\n",
    "import argparse\n",
    "from collections import namedtuple\n",
    "from gensim.utils import smart_open, to_utf8, tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n",
    "from nltk.util import ngrams\n",
    "import random,time\n",
    "import datetime\n",
    "from itertools import *\n",
    "from progressbar import ProgressBar, Percentage, Bar, ETA\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "date = str(now.year) + \"-\" + str(now.month) + \"-\" + str(now.day)\n",
    "path = os.getcwd()\n",
    "reload(sys)\n",
    "d = enchant.Dict(\"en_US\")\n",
    "negative = []\n",
    "positive = []\n",
    "neutral = []\n",
    "compounded = []\n",
    "total1=[]\n",
    "rev_id=[]\n",
    "subject=[]\n",
    "alt_subject=[]\n",
    "upd =0\n",
    "f = 'vader_sentiment_lexicon.txt'\n",
    "\n",
    "##########################################################################################################################################################################\n",
    "############################ all the relevant functions ##################################################################################################################\n",
    "def sentiment(text):\n",
    "    \"\"\"\n",
    "    Returns a float for sentiment strength based on the input text.\n",
    "    Positive values are positive valence, negative value are negative valence.\n",
    "    \"\"\"\n",
    "    text = text.lower().replace('\"','')\n",
    "    test =text.split(' ')\n",
    "    length = len(test)\n",
    "    #print length\n",
    "    for i in range (0,len(test)-1):\n",
    "        if '/' in test[i]:\n",
    "            test[i]=test[i].split('/')[0]\n",
    "\n",
    "        if test[i] == \"five\" or test[i] == \"one\" or test[i] == \"four\" or test[i] == \"three\" or test[i] == \"two\" or test[i] == \"5\" or test[i] == \"4\" or test[i] == \"3\" or test[i] == \"2\" or test[i] == \"1\" or test[i] == \"no\" or test[i] == \"open\" :\n",
    "            if test[i+1] =='star' or test[i+1] == 'stars' or test[i+1] == 'more'or test[i+1] == 'pack'or test[i+1] == 'packet'or test[i+1] == 'package':\n",
    "                test[i]=test[i] + '-' + test[i+1]\n",
    "                test [i+1]=''\n",
    "        if test[i] == \"stomach\" or test[i] == \"loose\":\n",
    "            if test[i+1] =='ace' or test[i+1] == 'motion':\n",
    "                test[i]=test[i] + '-' + test[i+1]\n",
    "                test [i+1]=''\n",
    "        if test[i] == \"use\":\n",
    "            if test[i+1] =='less' or test[i+1] == 'full' or test[i+1] == 'les' or test[i+1] == 'lss' or test[i+1] == 'ls':\n",
    "                test[i]=test[i] + '-' + test[i+1]\n",
    "                test [i+1]=''\n",
    "\n",
    "        if test[i] == \"die\" :\n",
    "            if test[i+1] =='to':\n",
    "                test[i]=test[i] + '-' + test[i+1]\n",
    "                if test[i+2]:\n",
    "                    test[i]=test[i] + '-' + test[i+2]\n",
    "                    test [i+2]=''\n",
    "        try:\n",
    "            if not d.check(test[i]):\n",
    "                test[i]=d.suggest(test[i])[0]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    text = \"  \".join(test)\n",
    "    text = text.strip()\n",
    "    wordsAndEmoticons = str(text).split() #doesn't separate words from adjacent punctuation (keeps emoticons & contractions)\n",
    "#    print wordsAndEmoticons\n",
    "    text_mod = text #regex_remove_punctuation.sub('', text) # removes punctuation (but loses emoticons & contractions)\n",
    " #   print text_mod\n",
    "    wordsOnly = str(text_mod).split()\n",
    " #   print wordsOnly\n",
    "    # get rid of empty items or single letter \"words\" like 'a' and 'I' from wordsOnly\n",
    "    for word in wordsOnly:\n",
    "        if len(word) <= 1:\n",
    "            wordsOnly.remove(word)    \n",
    "    # now remove adjacent & redundant punctuation from [wordsAndEmoticons] while keeping emoticons and contractions\n",
    "    puncList = [\".\", \"!\", \"?\", \",\", \";\", \":\", \"-\", \"'\", \"\\\"\", \n",
    "                \"!!\", \"!!!\", \"??\", \"???\", \"?!?\", \"!?!\", \"?!?!\", \"!?!?\"] \n",
    "    for word in wordsOnly:\n",
    "        for p in puncList:\n",
    "            pword = p + word\n",
    "            x1 = wordsAndEmoticons.count(pword)\n",
    "            while x1 > 0:\n",
    "                i = wordsAndEmoticons.index(pword)\n",
    "                wordsAndEmoticons.remove(pword)\n",
    "                wordsAndEmoticons.insert(i, word)\n",
    "                x1 = wordsAndEmoticons.count(pword)\n",
    "            \n",
    "            wordp = word + p\n",
    "            x2 = wordsAndEmoticons.count(wordp)\n",
    "            while x2 > 0:\n",
    "                i = wordsAndEmoticons.index(wordp)\n",
    "                wordsAndEmoticons.remove(wordp)\n",
    "                wordsAndEmoticons.insert(i, word)\n",
    "                x2 = wordsAndEmoticons.count(wordp)\n",
    "\n",
    "    # get rid of residual empty items or single letter \"words\" like 'a' and 'I' from wordsAndEmoticons\n",
    "    for word in wordsAndEmoticons:\n",
    "        if len(word) <= 1:\n",
    "            wordsAndEmoticons.remove(word)\n",
    "    word_len= len(wordsAndEmoticons)\n",
    "    #print wordsAndEmoticons   \n",
    "    # remove stopwords from [wordsAndEmoticons]\n",
    "    #stopwords = [str(word).strip() for word in open('stopwords.txt')]\n",
    "    #for word in wordsAndEmoticons:\n",
    "    #    if word in stopwords:\n",
    "    #        wordsAndEmoticons.remove(word)\n",
    "    \n",
    "    # check for negation\n",
    "    negate = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    "              \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    "              \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    "              \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    "              \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\",\"no\", \"nothing\", \"nowhere\", \n",
    "              \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    "              \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",  \n",
    "              \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\",\"less\"]\n",
    "    def negated(list, nWords=[], includeNT=True):\n",
    "        nWords.extend(negate)\n",
    "        #print list\n",
    "        for word in nWords:\n",
    "            if word in list:\n",
    "                #print word\n",
    "                return True\n",
    "        if includeNT:\n",
    "            for word in list:\n",
    "                if \"n't\" in word:\n",
    "                    return True\n",
    "        if \"least\" in list:\n",
    "            i = list.index(\"least\")\n",
    "            if i > 0 and list[i-1] != \"at\":\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def normalize(score, alpha=1):\n",
    "        # normalize the score to be between -1 and 1 using an alpha that approximates the max expected value \n",
    "        normScore = score/math.sqrt( ((score*score) + alpha) )\n",
    "        return normScore\n",
    "    \n",
    "    def wildCardMatch(patternWithWildcard, listOfStringsToMatchAgainst):\n",
    "        listOfMatches = fnmatch.filter(listOfStringsToMatchAgainst, patternWithWildcard)\n",
    "        return listOfMatches\n",
    "        \n",
    "    \n",
    "    def isALLCAP_differential(wordList):\n",
    "        countALLCAPS= 0\n",
    "        for w in wordList:\n",
    "            if str(w).isupper(): \n",
    "                countALLCAPS += 1\n",
    "        cap_differential = len(wordList) - countALLCAPS\n",
    "        if cap_differential > 0 and cap_differential < len(wordList):\n",
    "            isDiff = True\n",
    "        else: isDiff = False\n",
    "        return isDiff\n",
    "    isCap_diff = isALLCAP_differential(wordsAndEmoticons)\n",
    "    \n",
    "    b_incr = 0.293 #(empirically derived mean sentiment intensity rating increase for booster words)\n",
    "    b_decr = -0.293\n",
    "    # booster/dampener 'intensifiers' or 'degree adverbs' http://en.wiktionary.org/wiki/Category:English_degree_adverbs\n",
    "    booster_dict = {\"absolutely\": b_incr, \"amazingly\": b_incr, \"awfully\": b_incr, \"completely\": b_incr, \"considerably\": b_incr, \n",
    "                    \"decidedly\": b_incr, \"deeply\": b_incr, \"effing\": b_incr, \"enormously\": b_incr, \n",
    "                    \"entirely\": b_incr, \"especially\": b_incr, \"exceptionally\": b_incr, \"extremely\": b_incr,\n",
    "                    \"fabulously\": b_incr, \"flipping\": b_incr, \"flippin\": b_incr, \n",
    "                    \"fricking\": b_incr, \"frickin\": b_incr, \"frigging\": b_incr, \"friggin\": b_incr, \"fully\": b_incr, \"fucking\": b_incr, \n",
    "                    \"greatly\": b_incr, \"hella\": b_incr, \"highly\": b_incr, \"hugely\": b_incr, \"incredibly\": b_incr, \n",
    "                    \"intensely\": b_incr, \"majorly\": b_incr, \"more\": b_incr, \"most\": b_incr, \"must\": b_incr, \"particularly\": b_incr, \n",
    "                    \"purely\": b_incr,\"please\": b_incr, \"quite\": b_incr, \"really\": b_incr, \"remarkably\": b_incr, \n",
    "                    \"so\": b_incr,  \"substantially\": b_incr, \"damn\": b_incr, \n",
    "                    \"thoroughly\": b_incr, \"totally\": b_incr, \"tremendously\": b_incr, \n",
    "                    \"uber\": b_incr, \"unbelievably\": b_incr, \"unusually\": b_incr, \"utterly\": b_incr, \n",
    "                    \"very\": b_incr,\"big\":b_incr,\"help\":b_incr,\"heck\":b_incr,\"too\":b_incr,\"lots\":b_incr,\"atall\":b_incr,\n",
    "                    \n",
    "                    \"almost\": b_decr, \"barely\": b_decr, \"hardly\": b_decr, \"just enough\": b_decr, \n",
    "                    \"kind of\": b_decr, \"kinda\": b_decr, \"kindof\": b_decr, \"kind-of\": b_decr,\n",
    "                    \"less\": b_decr, \"little\": b_decr, \"marginally\": b_decr, \"occasionally\": b_decr, \"partly\": b_decr, \n",
    "                    \"scarcely\": b_decr, \"slightly\": b_decr, \"somewhat\": b_decr, \n",
    "                    \"sort of\": b_decr, \"sorta\": b_decr, \"sortof\": b_decr, \"sort-of\": b_decr,\"rip\": b_decr}\n",
    "    sentiments = []\n",
    " #   print wordsAndEmoticons\n",
    "    for item in wordsAndEmoticons:\n",
    "       # print (item)\n",
    "        v = 0\n",
    "        i = wordsAndEmoticons.index(item)\n",
    "        if (i < len(wordsAndEmoticons)-1 and str(item).lower() == \"kind\" and \\\n",
    "           str(wordsAndEmoticons[i+1]).lower() == \"of\") or str(item).lower() in booster_dict:\n",
    "            sentiments.append(v)\n",
    "            continue\n",
    "        item_lowercase = str(item).lower()\n",
    "        \n",
    "        if  item_lowercase in word_valence_dict:\n",
    "           # print (\"item in valence lexion file--->\",item_lowercase)\n",
    "            v = float(word_valence_dict[item_lowercase])\n",
    "            #print (\"val\", v)\n",
    "            c_incr = 0.733 #(empirically derived mean sentiment intensity rating increase for using ALLCAPs to emphasize a word)\n",
    "            if str(item).isupper() and isCap_diff:\n",
    "                if v > 0: v += c_incr\n",
    "                else: v -= c_incr\n",
    "            def scalar_inc_dec(word, valence):\n",
    "                scalar = 0.0\n",
    "                \n",
    "                word_lower = str(word).lower()\n",
    "                if word_lower in booster_dict:\n",
    "                    #print (\"words in scalar-->\",word_lower)\n",
    "                    scalar = booster_dict[word_lower]\n",
    "                    if valence < 0: scalar *= -1\n",
    "                    if str(word).isupper() and isCap_diff:\n",
    "                        if valence > 0: scalar += c_incr\n",
    "                        else:  scalar -= c_incr\n",
    " #               print (\"scal-->\",scalar) \n",
    "                return scalar\n",
    "            n_scalar = -1.0\n",
    "            if (i+1)<word_len:\n",
    "                if str(wordsAndEmoticons[i+1]).lower() in word_valence_dict and negated([wordsAndEmoticons[i]]) :\n",
    "                    print (\"insde\")\n",
    "\n",
    "            #print (\"previous-->\",wordsAndEmoticons[i-1])                               \n",
    "            try:\n",
    "                if i > 0 and str(wordsAndEmoticons[i-1]).lower() not in word_valence_dict and (i+1)<=word_len:\n",
    "                    s1 = scalar_inc_dec(wordsAndEmoticons[i-1], v)\n",
    "                    v = v+s1\n",
    "                    \n",
    "                    if negated([wordsAndEmoticons[i-1]]):\n",
    "                        \n",
    "                        v = v*n_scalar\n",
    "                        #print (\"insie\",v)\n",
    "                \n",
    "            except:\n",
    "                if i > 0 and str(wordsAndEmoticons[i-1]).lower() not in word_valence_dict and (i+1)<word_len:\n",
    "                    s1 = scalar_inc_dec(wordsAndEmoticons[i-1], v)\n",
    "                    v = v+s1\n",
    "                    \n",
    "                    if negated([wordsAndEmoticons[i-1]]):\n",
    "                        v = v*n_scalar\n",
    "                    #print (\"inside 2\",v)\n",
    "                \n",
    "            if i > 1 and str(wordsAndEmoticons[i-2]).lower() not in word_valence_dict and (i+1)<=word_len:\n",
    "                s2 = scalar_inc_dec(wordsAndEmoticons[i-2], v)\n",
    "                if s2 != 0: s2 = s2*0.95\n",
    "                v = v+s2\n",
    "                if wordsAndEmoticons[i-2] == \"never\" and (wordsAndEmoticons[i-1] == \"so\" or wordsAndEmoticons[i-1] == \"this\"): \n",
    "                    v = v*1.5                    \n",
    "                \n",
    "                elif negated([wordsAndEmoticons[i-2]]):\n",
    "                    v = v*n_scalar\n",
    "            if i > 2 and str(wordsAndEmoticons[i-3]).lower() not in word_valence_dict and (i+1)<word_len:\n",
    "                s3 = scalar_inc_dec(wordsAndEmoticons[i-3], v)\n",
    "                if s3 != 0: s3 = s3*0.9\n",
    "                v = v+s3\n",
    "                if wordsAndEmoticons[i-3] == \"never\" and \\\n",
    "                   (wordsAndEmoticons[i-2] == \"so\" or wordsAndEmoticons[i-2] == \"this\") or \\\n",
    "                   (wordsAndEmoticons[i-1] == \"so\" or wordsAndEmoticons[i-1] == \"this\"):\n",
    "                    v = v*1.25\n",
    "                elif negated([wordsAndEmoticons[i-3]]): v = v*n_scalar\n",
    "                \n",
    "                # check for special case idioms using a sentiment-laden keyword known to SAGE\n",
    "                special_case_idioms = {\"the shit\": 3, \"the bomb\": 3, \"bad ass\": 1.5, \"yeah right\": -2, \n",
    "                                       \"cut the mustard\": 2, \"kiss of death\": -1.5, \"hand to mouth\": -2}\n",
    "                # future work: consider other sentiment-laden idioms\n",
    "                #other_idioms = {\"back handed\": -2, \"blow smoke\": -2, \"blowing smoke\": -2, \"upper hand\": 1, \"break a leg\": 2, \n",
    "                #                \"cooking with gas\": 2, \"in the black\": 2, \"in the red\": -2, \"on the ball\": 2,\"under the weather\": -2}\n",
    "                onezero = \"{} {}\".format(str(wordsAndEmoticons[i-1]), str(wordsAndEmoticons[i]))\n",
    "                twoonezero = \"{} {}\".format(str(wordsAndEmoticons[i-2]), str(wordsAndEmoticons[i-1]), str(wordsAndEmoticons[i]))\n",
    "                twoone = \"{} {}\".format(str(wordsAndEmoticons[i-2]), str(wordsAndEmoticons[i-1]))\n",
    "                threetwoone = \"{} {} {}\".format(str(wordsAndEmoticons[i-3]), str(wordsAndEmoticons[i-2]), str(wordsAndEmoticons[i-1]))\n",
    "                threetwo = \"{} {}\".format(str(wordsAndEmoticons[i-3]), str(wordsAndEmoticons[i-2]))                    \n",
    "                if onezero in special_case_idioms: v = special_case_idioms[onezero]\n",
    "                elif twoonezero in special_case_idioms: v = special_case_idioms[twoonezero]\n",
    "                elif twoone in special_case_idioms: v = special_case_idioms[twoone]\n",
    "                elif threetwoone in special_case_idioms: v = special_case_idioms[threetwoone]\n",
    "                elif threetwo in special_case_idioms: v = special_case_idioms[threetwo]\n",
    "                if len(wordsAndEmoticons)-1 > i:\n",
    "                    zeroone = \"{} {}\".format(str(wordsAndEmoticons[i]), str(wordsAndEmoticons[i+1]))\n",
    "                    if zeroone in special_case_idioms: v = special_case_idioms[zeroone]\n",
    "                if len(wordsAndEmoticons)-1 > i+1:\n",
    "                    zeroonetwo = \"{} {}\".format(str(wordsAndEmoticons[i]), str(wordsAndEmoticons[i+1]), str(wordsAndEmoticons[i+2]))\n",
    "                    if zeroonetwo in special_case_idioms: v = special_case_idioms[zeroonetwo]\n",
    "                \n",
    "                # check for booster/dampener bi-grams such as 'sort of' or 'kind of'\n",
    "                if threetwo in booster_dict or twoone in booster_dict:\n",
    "                    v = v+b_decr\n",
    "            \n",
    "            # check for negation case using \"least\"\n",
    "            if i > 1 and str(wordsAndEmoticons[i-1]).lower() not in word_valence_dict \\\n",
    "                and str(wordsAndEmoticons[i-1]).lower() == \"least\" and (i+1)<word_len:\n",
    "                if (str(wordsAndEmoticons[i-2]).lower() != \"at\" and str(wordsAndEmoticons[i-2]).lower() != \"very\") :\n",
    "                    v = v*n_scalar\n",
    "            elif i > 0 and str(wordsAndEmoticons[i-1]).lower() not in word_valence_dict \\\n",
    "                and str(wordsAndEmoticons[i-1]).lower() == \"least\" and (i+1)<word_len:\n",
    "                v = v*n_scalar\n",
    "       # print (v)\n",
    "        sentiments.append(v) \n",
    "            \n",
    "    # check for modification in sentiment due to contrastive conjunction 'but'\n",
    "    if 'but' in wordsAndEmoticons or 'BUT' in wordsAndEmoticons:#\n",
    "        try: bi = wordsAndEmoticons.index('but')\n",
    "        except: bi = wordsAndEmoticons.index('BUT')\n",
    "        for s in sentiments:\n",
    "            si = sentiments.index(s)\n",
    "            if si <bi :\n",
    "                sentiments.pop(si)\n",
    "                sentiments.insert(si, s*0.8)\n",
    "            elif si>bi :\n",
    "                #print (\"si-->\",si)\n",
    "                sentiments.pop(si)\n",
    "                sentiments.insert(si, s*1.2)\n",
    "    if 'request' in wordsAndEmoticons or 'REQUEST' in wordsAndEmoticons:#\n",
    "        try: bi = wordsAndEmoticons.index('request')\n",
    "        except: bi = wordsAndEmoticons.index('request')\n",
    "        for s in sentiments:\n",
    "            si = sentiments.index(s)\n",
    "            if si <bi :\n",
    "                sentiments.pop(si)\n",
    "                sentiments.insert(si, s*1.8)\n",
    "    \n",
    "               \n",
    " #       print (\"sent-->\",sentiments)\n",
    "                \n",
    "    if sentiments:\n",
    "        \n",
    "        sum_s = float(sum(sentiments))\n",
    "        #print sentiments, sum_s\n",
    "        \n",
    "        # check for added emphasis resulting from exclamation points (up to 4 of them)\n",
    "        ep_count = str(text).count(\"!\")\n",
    "        if ep_count > 4: ep_count = 4\n",
    "        ep_amplifier = ep_count*0.292 #(empirically derived mean sentiment intensity rating increase for exclamation points)\n",
    "        if sum_s > 0:  sum_s += ep_amplifier\n",
    "        elif  sum_s < 0: sum_s -= ep_amplifier\n",
    "        \n",
    "        # check for added emphasis resulting from question marks (2 or 3+)\n",
    "        qm_count = str(text).count(\"?\")\n",
    "        qm_amplifier = 0\n",
    "        if qm_count > 1:\n",
    "            if qm_count <= 3: qm_amplifier = qm_count*0.18\n",
    "            else: qm_amplifier = 0.96\n",
    "            if sum_s > 0:  sum_s += qm_amplifier\n",
    "            elif  sum_s < 0: sum_s -= qm_amplifier\n",
    "       # print (sum_s)\n",
    "        compound = normalize(sum_s)\n",
    "        #print (compound)\n",
    "        # want separate positive versus negative sentiment scores\n",
    "        pos_sum = 0.0\n",
    "        neg_sum = 0.0\n",
    "        neu_count = 0\n",
    "        total = 1\n",
    "        tot = 0\n",
    "        for sentiment_score in sentiments:\n",
    "            if sentiment_score > 0:\n",
    "                pos_sum += (float(sentiment_score) +1) # compensates for neutral words that are counted as 1\n",
    "            if sentiment_score < 0:\n",
    "                neg_sum += (float(sentiment_score) -1) # when used with math.fabs(), compensates for neutrals\n",
    "            if sentiment_score == 0:\n",
    "                neu_count += 1\n",
    "        \n",
    "        if pos_sum > math.fabs(neg_sum): pos_sum += (ep_amplifier+qm_amplifier)\n",
    "        elif pos_sum < math.fabs(neg_sum): neg_sum -= (ep_amplifier+qm_amplifier)\n",
    "\n",
    "        neu_count=normalize(neu_count)\n",
    "        total = (math.fabs(pos_sum) + math.fabs(neg_sum))\n",
    "        if total == 0 :\n",
    "            total = 1\n",
    "#        print pos_sum ,\" \",neg_sum\n",
    "        pos = (math.fabs(pos_sum / total))\n",
    "        neg = (math.fabs(neg_sum / total))\n",
    "        neu = normalize((neu_count/total))\n",
    "    else:\n",
    "        compound = 0.0; pos = 0.0; neg = 0.0; neu = 0.0\n",
    "#    print neg\n",
    "    tot = pos+(-1)*neg+neu\n",
    "\n",
    "        \n",
    "    s = {\"neg\" : round(neg, 3), \n",
    "         \"neu\" : round(neu, 3),\n",
    "         \"pos\" : round(pos, 3),\n",
    "         \"compound\" : round(compound, 4),\n",
    "         \"total\" : round(tot,3)}\n",
    "\n",
    "    return s\n",
    "\n",
    "import unicodedata\n",
    "def vader(x):\n",
    "    global upd\n",
    "    try:\n",
    "        x=x.encode('utf-8')\n",
    "        sentence = str(x).encode('ascii','ignore').decode('ascii')\n",
    "        i=0\n",
    "        temp_p=0\n",
    "        temp_n=0\n",
    "        temp_c=0\n",
    "        lines_list = re.split(r'[.!?]+', str(sentence))\n",
    "        for sent in lines_list:\n",
    "            sent=sent.strip().lower()\n",
    "            if sent == '':\n",
    "                continue\n",
    "            else:\n",
    "                ss = sentiment(sent)\n",
    "                if not all(value == 0 for value in ss.values()):\n",
    "                    i=i+1\n",
    "                temp_p=temp_p+ss['pos']\n",
    "                temp_n=temp_n+ss['neg']\n",
    "                temp_c=temp_c+ss['compound']\n",
    "        if i==0:\n",
    "            i=1\n",
    "        if len(str(sentence).split(\" \")) <4:\n",
    "            i=i*2\n",
    "        tc=float(temp_c/float(i))*100\n",
    "        tp=float(temp_p/float(i))*100\n",
    "        tn = float(temp_n/float(i))*100\n",
    "    except:\n",
    "        tc = 0\n",
    "    upd = upd+1\n",
    "    pbar.update(upd)\n",
    "    return tc\n",
    "\n",
    "\n",
    "def get_ngrams(text ):\n",
    "    list1 =[]\n",
    "    for i in range(1,len(text.split(' '))):\n",
    "        n_grams = ngrams(word_tokenize(text), i)\n",
    "        list1.append([ ' '.join(grams) for grams in n_grams])\n",
    "    list1=list(chain.from_iterable(list1))\n",
    "    return list1\n",
    "def aspect_finder(review_file):\n",
    "    aspect_out =[]\n",
    "    global upd\n",
    "    for row in review_file.to_dict('records'):\n",
    "        \n",
    "        rcs = 100.0\n",
    "        dead = ['died','death','dying','hospit','killed','ill','vomit','vomited','vomitted','vomits','blood','blud','blod']\n",
    "        aspect = []\n",
    "        aspect_sent =[]\n",
    "        aspect_keyword = []\n",
    "        sent_keyword=[]\n",
    "        flag =0\n",
    "        #try:\n",
    "        temp_text = row['review_text'].encode('ascii','ignore')\n",
    "        text = str(temp_text).lower()\n",
    "        txt =get_ngrams(text)\n",
    "        ques = set(txt)&set(question_words) \n",
    "        ques = sorted(ques, key = lambda k : txt.index(k))\n",
    "        if any(ques):\n",
    "            flag =1\n",
    "            math.ceil(random.uniform(78.4, 85.9)*100)/100\n",
    "\n",
    "        die = set(txt)&set(dead) \n",
    "        die = sorted(die, key = lambda k : txt.index(k))\n",
    "        if any(die):\n",
    "            flag =2\n",
    "            rcs = math.ceil(random.uniform(95.4, 99.9)*100)/100\n",
    "        aspects  = aspects_df.aspect_name.unique() \n",
    "\n",
    "        for asp in aspects:\n",
    "            t = aspects_df[aspects_df.aspect_name == asp]\n",
    "            keyword = t['keyword']\n",
    "            keyword =[i.replace('\\n','').split('\\r', 1)[0] for i in keyword]\n",
    "            list1 = set(keyword)&set(txt) \n",
    "            list2 = sorted(list1, key = lambda k : txt.index(k))\n",
    "            if any(list2) :\n",
    "                aspect.append(asp)\n",
    "                aspect_keyword.append(list2)\n",
    "        aspect_keyword=list(chain.from_iterable(aspect_keyword))\n",
    "        aspect_keyword=[i.replace('\\n','').split('\\r', 1)[0] for i in aspect_keyword]\n",
    "        match = set(aspect_keyword)&set(dont_reply_aspects) \n",
    "        match = sorted(match, key = lambda k : aspect_keyword.index(k))\n",
    "        if any(match):\n",
    "            rcs = float(rcs - float(len(match)*.65))\n",
    "            if len(match) == len(aspect_keyword):\n",
    "                aspect.append('dont_reply')\n",
    "                flag = 3\n",
    "                rcs = 0.0\n",
    "        aspects_sent  = aspect_sent_df.aspect_name.unique() \n",
    "\n",
    "        for asp in aspects_sent:\n",
    "            t = aspect_sent_df[aspect_sent_df.aspect_name == asp]\n",
    "            keyword = t['keyword']\n",
    "            keyword =[i.replace('\\n','').split('\\r', 1)[0] for i in keyword]\n",
    "            list1 = set(keyword)&set(txt) \n",
    "            list2 = sorted(list1, key = lambda k : txt.index(k))\n",
    "            if any(list2) :\n",
    "                aspect.append(asp)\n",
    "                aspect_sent.append(list2)\n",
    "        sent  = sentiment_df.keyword.unique() \n",
    "\n",
    "        keyword = sent\n",
    "        keyword =[i.replace('\\n','').split('\\r', 1)[0] for i in keyword]\n",
    "        list1 = set(keyword)&set(txt) \n",
    "        list2 = sorted(list1, key = lambda k : txt.index(k))\n",
    "        if any(list2) :\n",
    "            sent_keyword.append(list2)\n",
    "\n",
    "        if len(temp_text) < 80 and len(aspect_keyword) < 1:\n",
    "            #aspects_missed = expected - actual\n",
    "            aspects_missed = 1 - len(aspect_keyword)\n",
    "        elif len(temp_text) >= 80 and len(temp_text)< 180 and len(aspect_keyword) < 2:\n",
    "            aspects_missed = 2 - len(aspect_keyword)\n",
    "        elif len(temp_text) >= 180 and len(temp_text) < 450 and len(aspect_keyword) < 3:\n",
    "            aspects_missed = 3 - len(aspect_keyword)\n",
    "        elif len(temp_text) >= 450 and len(aspect_keyword) < 4:\n",
    "            aspects_missed = 4 - len(aspect_keyword)\n",
    "        else:\n",
    "            aspects_missed = 0\n",
    "        rcs = rcs - 4*aspects_missed*.05\n",
    "        row['brand_name'] = unicodedata.normalize('NFKD', row['brand_name']).encode('ascii','ignore')\n",
    "        row['reviewer_name'] = unicodedata.normalize('NFKD', row['reviewer_name']).encode('ascii','ignore')\n",
    "        if row['brand_name'] in row['reviewer_name']:\n",
    "            flag = 5\n",
    "            rcs=0\n",
    "        row['aspect']=aspect\n",
    "        row['aspect_sentiment']=aspect_sent\n",
    "        row['aspect_keyword'] = aspect_keyword\n",
    "        row['sentiment_keyword'] = sent_keyword\n",
    " \n",
    "        row['flag'] = flag\n",
    "        score = row['confidence_score']\n",
    "        if int(score) <= 0:\n",
    "            row['polarity'] = 'negative'\n",
    "        if int(score) > 0:\n",
    "            row['polarity'] = 'positive'\n",
    "        if int(flag) == 1:\n",
    "            row['polarity'] = 'neutral'\n",
    "        row['response_confidence_score'] = rcs\n",
    "        aspect_out.append(row)\n",
    "        upd = upd+1\n",
    "        pbar.update(upd)\n",
    "    return aspect_out\n",
    "def replier(x):\n",
    "    global upd\n",
    "    row = dict(x)\n",
    "    replies_hot = replies_dict[replies_dict.aspect == 'hot-clusters']\n",
    "    brand = row['brand_name'].lower()\n",
    "    conf_score = row['confidence_score']\n",
    "    \n",
    "    \n",
    "    if brand == 'whiskas' or brand == 'sheba':\n",
    "       # print brand\n",
    "        faq = 'https://www.whiskas.com/Home/Faqs'\n",
    "        #pet = 'cat'\n",
    "        email = 'whiskas.india@effem.com'\n",
    "       # print email\n",
    "    else:\n",
    "        faq = 'https://www.pedigree.com/faqs'\n",
    "        #pet = 'dog'\n",
    "        email = 'pedigree.india@effem.com'\n",
    "    pet = 'pet'\n",
    "\n",
    "    reply =\"\"\n",
    "    review_text = row['review_text']\n",
    "   \n",
    "    try:\n",
    "        reviewer_name = str(row['reviewer_name'])\n",
    "       #print reviewer_name\n",
    "        reviewer_name = str(reviewer_name).encode('utf-8').encode('ascii','ignore').decode('ascii')\n",
    "    except:\n",
    "        reviewer_name = 'Customer'\n",
    "    print reviewer_name\n",
    "    star_rating = row['star_rating']\n",
    "    flag = row['flag']\n",
    "    if conf_score < 0:\n",
    "        if flag == 0 or flag =='3':\n",
    "            reply = random.choice(list(replies_hot.template_response))\n",
    "    else:\n",
    "        try:\n",
    "            replies_overall = replies_dict[replies_dict.aspect == row['aspect'][0]]\n",
    "            replies_overall = replies_dict[replies_dict.sentiment_type == row['polarity']]\n",
    "            reply =random.choice(list(replies_overall.template_response))\n",
    "        except:\n",
    "            if row['polarity'] == 'positive':\n",
    "                replies_overall = replies_dict[replies_dict.aspect == 'product-overall-5stars']\n",
    "                reply =random.choice(list(replies_overall.template_response))\n",
    "            else:\n",
    "                reply = \"Thanks for the feedback, it will certainly help us improve.\"\n",
    "    if \"customer name\" in reply:\n",
    "        reply=reply.replace('customer name',str(reviewer_name))\n",
    "    elif \"customer\" in reply or \"Customer\" in reply:\n",
    "        reply=reply.replace('customer',str(reviewer_name))\n",
    "        reply=reply.replace('Customer',str(reviewer_name))\n",
    "    else:\n",
    "        x = \"Dear \"+ str(reviewer_name)\n",
    "        reply = x +', '+reply + \" Please feel free to reach us at our toll free number 1800-4071-12121 or write to us at \"+email+\" for any further assistance. Regards Team \"+row['brand_name']+\".\"\n",
    "    if flag == 1:\n",
    "        reply = \"Dear \"+reviewer_name+\", For any queries and facts regarding your \"+pet+\" please refer to the FAQs at our official site \"+faq+\". Please feel free to reach us at our toll free number 1800-4071-12121 or write to us at \"+email+\" for any further assistance. Regards Team \"+row['brand_name']+\".\"\n",
    "    if flag == 5:\n",
    "        reply = \"\"\n",
    "    if not row['sentiment_keyword'] and row['source_name'] == 'Facebook' and row['flag'] == 0 or not row['aspect'] and not row['aspect_sentiment'] and row['source_name'] == 'Facebook':\n",
    "        reply = ''\n",
    "    if not row['aspect'] and not row['aspect_sentiment'] and flag == 0:\n",
    "        reply = \"Dear \"+reviewer_name+\", Thanks for your valuable feedback. \"\n",
    "    if 'confused' in row['review_text'] or 'confuse' in row['review_text']:\n",
    "        reply = \"Dear \"+reviewer_name+\", For any queries and facts regarding your \"+pet+\" please refer to the FAQs at our official site \"+faq+\". Please feel free to reach us at our toll free number 1800-4071-12121 or write to us at \"+email+\" for any further assistance. Regards Team \"+row['brand_name']+\".\"\n",
    "    if flag == 2:  \n",
    "        reply = \"Dear \"+reviewer_name+\", We are deeply saddened to hear about your \"+pet+\" . We request you to share your contact details so that we can assist you ASAP on our toll free number 1800-4071-12121 or write to us at \"+email+\". Regards Team \"+row['brand_name']+\".\"\n",
    "    if pet == 'cat':\n",
    "        #reply = re.sub(r\"\\bdog\\b\",\"cat\",reply)\n",
    "        reply = reply.replace('pedigree',row['brand_name'])\n",
    "        reply = reply.replace('Pedigree',row['brand_name'])\n",
    "#     reply = re.sub(r\"\\bpet\\b\",pet,reply)\n",
    "#     reply = re.sub(r\"\\bpet.\\b\",pet+\".\",reply)\n",
    "#     reply = re.sub(r\"\\bpet!\\b\",pet+\"!\",reply)\n",
    "#     reply = re.sub(r\"\\bpet?\\b\",pet+\"?\",reply)\n",
    "#     reply = re.sub(r\"\\bpet,\\b\",pet+\",\",reply)\n",
    "    list_rev = [\"thanks for the review\",'thank you for your review','thank you for the review','good luck']\n",
    "    try:\n",
    "        if any(substring in str(row['review_text']).lower() for substring in list_rev):\n",
    "            reply = \"Dear \"+reviewer_name+\", Thanks for your valuable feedback. \"\n",
    "    except:\n",
    "        pass\n",
    "    #print brand,'---->'\n",
    "#     upd = upd+1\n",
    "#     pbar.update(upd)\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################################################\n",
    "###############################################################################################################################################################################\n",
    "start = time.time()\n",
    "#####################################################################################\n",
    "############################ reading reviews file ###################################\n",
    "review_file = pd.read_csv('source_reviews_for_response_generation_2018-05-12 17_19_16.919510.csv',encoding='utf-8',quoting=csv.QUOTE_ALL)\n",
    "review_file.dropna(subset=['review_text'],inplace=True) \n",
    "#review_file = review_file[review_file.source_review_id == 'Mx161SNL9KB1S36']\n",
    "\n",
    "#####################################################################################\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "####### finding confidence/sentiment score for each review using vader ##############\n",
    "leng = review_file.shape[0]\n",
    "widgets = [\"Finding Sentiment Score using Vader: \", Percentage(), ' ', Bar(), ' ', ETA()]\n",
    "pbar = ProgressBar(widgets=widgets, maxval=leng).start()\n",
    "word_valence_dict = dict(map(lambda (w, m): (w, float(m)), [wmsr.strip().split('\\t')[0:2] for wmsr in open(f) ]))\n",
    "regex_remove_punctuation = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "review_file['confidence_score'] = review_file.review_text.apply(lambda row: vader(row))\n",
    "pbar.finish()\n",
    "upd = 0\n",
    "#####################################################################################\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "###################### reading aspects and sentiment file ###########################\n",
    "\n",
    "aspects_file = pd.read_csv('aspect_related_words.csv')\n",
    "sentiment_file = pd.read_csv('meta_words.txt',sep='~')\n",
    "replies_dict = pd.read_csv('response_templates.csv')\n",
    "dont_reply = replies_dict[replies_dict.status == 'DR']\n",
    "dont_reply_aspect_asp = list(dont_reply.aspect.unique())\n",
    "dont_reply_aspects = aspects_file[aspects_file['aspect_name'].isin(dont_reply_aspect_asp)].keyword\n",
    "dont_reply_aspects=[i.replace('\\n','').split('\\r', 1)[0] for i in dont_reply_aspects]\n",
    "dont_reply_aspects= filter(None, dont_reply_aspects)\n",
    "aspects_df = aspects_file[aspects_file.lexicon_type == 'A']\n",
    "aspect_sent_df  = aspects_file[aspects_file.lexicon_type == 'AS']\n",
    "sentiment_df = sentiment_file[sentiment_file.keyword_type == 'S']\n",
    "question_df = sentiment_file[sentiment_file.keyword_type == 'Q']\n",
    "question_words = question_df.keyword.unique()\n",
    "\n",
    "#####################################################################################\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "###################### finding aspects for each review #############################\n",
    "#widgets = [\"Finding Aspect words and Aspects: \", Percentage(), ' ', Bar(), ' ', ETA()]\n",
    "#pbar = ProgressBar(widgets=widgets, maxval=leng).start()\n",
    "aspect_out = aspect_finder(review_file)\n",
    "aspect_out=pd.DataFrame(aspect_out)\n",
    "#print aspect_out[aspect_out.source_review_id == '']\n",
    "#pbar.finish()\n",
    "upd =0 \n",
    "\n",
    "#####################################################################################\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "###################### finding response for each review #############################\n",
    "widgets = [\"Generating Responses: \", Percentage(), ' ', Bar(), ' ', ETA()]\n",
    "pbar = ProgressBar(widgets=widgets, maxval=leng).start()\n",
    "aspect_out=pd.DataFrame(aspect_out)\n",
    "aspect_out['response'] = aspect_out.apply(lambda row: replier(row),axis=1)\n",
    "aspect_out['confidence_score'] = np.where(aspect_out['flag'] == 2, -100,aspect_out['confidence_score'])\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_out = aspect_out[aspect_out.response!='']\n",
    "aspect_out = aspect_out[~aspect_out.reviewer_name.str.contains('Expert')]\n",
    "aspect_out.to_csv('responses_mars_pedigree_'+str(date)+'_all_headers.csv',quoting=csv.QUOTE_ALL,index=False,encoding='utf-8')\n",
    "final_resp = aspect_out\n",
    "aspect_out['response_2'] = \"\"\n",
    "aspect_out['response_3'] = \"\"\n",
    "aspect_out['degree'] = \"\"\n",
    "final_resp.id = final_resp.id.astype(int)\n",
    "cols = ['id','source_review_id','response','response_2','response_3','confidence_score','polarity','degree','response_confidence_score']\n",
    "final_resp = final_resp[cols]\n",
    "final_resp.dropna(subset=['source_review_id'],inplace=True) \n",
    "\n",
    "final_resp.to_csv('responses_mars_pedigree_'+str(date)+'_db_ingestion.txt',sep='~',quoting=csv.QUOTE_ALL,index=False,encoding='utf-8')\n",
    "\n",
    "#####################################################################################\n",
    "#####################################################################################\n",
    "end = time.time()\n",
    "print \"Time taken by code is :-\",end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "others = pd.DataFrame()\n",
    "out = []\n",
    "for id in ids:\n",
    "    ch1 = df[df.category_id == id]\n",
    "    aid = 1\n",
    "    if id == 33:\n",
    "        aid =5\n",
    "    if id == 35:\n",
    "        aid =7\n",
    "    if id == 32:\n",
    "        aid = 0\n",
    "    ch2 = ch1[ch1.aspect_id == aid]\n",
    "    others = pd.concat([others,ch2])\n",
    "    ch1 = ch1[ch1.aspect_id != aid]\n",
    "    for row in ch1.to_dict('records'):\n",
    "        flag =0\n",
    "        for row1 in ch2.to_dict('records'):\n",
    "            if re.search(r'\\b' + row1['keyword'] + r'\\b', row['keyword']):\n",
    "                row['others_keyword'] = row1['keyword']\n",
    "                row['others_lexicon_type'] = row1['lexicon_type']\n",
    "                row['others_id'] = row1['id']\n",
    "                row['others_sentiment_type'] = row1['sentiment_type']\n",
    "                out.append(row)\n",
    "                flag = 1\n",
    "        if flag ==0:\n",
    "            out.append(row)\n",
    "    print \"done\", id\n",
    "out = pd.DataFrame(out)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"Watch is awesome !!!!!!!!!! Only the box of the watch is not so good .but watch is good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =\"good\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "z = re.split('[;\\n\\r\\t?!.]',x)\n",
    "sent = ['awesome','good','great']\n",
    "for a in z:\n",
    "    a = a.lower()\n",
    "    for s in sent:\n",
    "        for i in re.findall(r''+y+'.*?'+s,a):\n",
    "            print i,len(i.split())-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watch is not so g 3\n",
      "watch is g 1\n"
     ]
    }
   ],
   "source": [
    "for i in re.findall(r''+y+'.*?g',x):\n",
    "    print i,len(i.split())-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "box of the watch 2\n"
     ]
    }
   ],
   "source": [
    "for i in re.findall(r'box.*?'+y,x):\n",
    "    print i,len(i.split())-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_word_length = [len(i.split())-2 for i in re.findall(r''+y+'.\\*?box',x)]\n",
    "b_word_length = [len(i.split())-2 for i in re.findall(r'box.\\*?'+y,x)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not so g']\n",
      "yres\n"
     ]
    }
   ],
   "source": [
    "a =re.findall(r'not.*?\\g',x.lower())\n",
    "print a\n",
    "if a:\n",
    "    print \"yres\"\n",
    "else:\n",
    "    print \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['awesome !!!!!!!!!! only the box of the watch is not so good']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'awesome.*?\\ '+y,x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'findany'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-f96f9f117325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://finance.yahoo.com/q?s=%s&q1=1\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mregex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'<span id=\"yfs_l84_%s\">(.?+)</span>'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"The price of\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" is\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'findany'"
     ]
    }
   ],
   "source": [
    "symbol =\"aapl\"\n",
    "url = \"http://finance.yahoo.com/q?s=%s&q1=1\" % symbol\n",
    "regex = '<span id=\"yfs_l84_%s\">(.?+)</span>' % symbol\n",
    "price = re.findany(regex, url)[0]\n",
    "print \"The price of\", symbol,\" is\", price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
